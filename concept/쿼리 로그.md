---
date: 2024-05-15
datetime: 2024-05-15 22:03:06
book: 데이터_품질의_비밀
page:
  - "60"
tags: 
references: 
aliases:
---
웨어 하우스에서 만들어진 변환에 대한 기록

쿼리 로그를 사용하면 아래와 같은 정보 확인 가능
- 누가 이 데이터에 접근하는가?
- upstream/downstream은 어디서 오는가?
- 평균적으로 이 특정 변환이 얼마나 자주 실행되는가?
- 몇 개의 행이 영향을 받는가?

대부분 데이터 웨어하우스 공급 업체의 **시스템 테이블**에 패키지로 제공됨
- Snowflake, [[QUERY_HISTORY]] table family
- Bigquery, AuditLogs Resource
- Redshift, [[STL_QUERY]] table family

일반적으로
- 며칠 간의 쿼리 기록만 저장
- 데이터 품질 이니셔티브에 필요한 것보다 훨씬 더 많은 정보 포함

즉, 데이터 품질 메트릭을 위해 쿼리 로그를 처리하는 강력한 솔루션은
- 사전 예방적으로
- 원하는 메트릭과 집계를
- **보다 영구적인 위치에 집계할 것**

snowflake와 redshift의 경우 다음과 같은 지표들이 존재함
- 쿼리를 실행한 사용자 ID
- 쿼리의 SQL 텍스트와 쿼리를 식별하는 hash
- 시작부터 결과까지 쿼리의 총 시간
- 오류 코드가 생성된 경우
- 쿼리의 입출력 사이즈(행 또는 바이트 단위)

수집된 메타데이터 활용시 다음 질문에 대한 답 가능
- 이 테이블은 어제 마지막으로 쿼리되었는가?
- 업데이트는 지금까지 흐름과 유사했는가? 아니면 패턴을 깨는가?
- 데이터 웨어하우스의 하루 적재량은 어느 정도인가?
- 두달 전보다 점진적으로 더 오래 걸리는가?
- 누가(또는 어떤 봇이) 이 리소스에 접근할 수 있는가?
