---
date: 2024-12-22
datetime: 2024-12-22 16:46:23
book: 데이터_거버넌스
page:
  - "33"
tags: 
references: 
aliases:
---
- [[데이터 거버넌스]]란

# 데이터 거버넌스는 무엇에 관여하는가
- 클라우드의 이동, 컴퓨터 성능 향상 -> 빅데이터 분석이 가능해짐
	- 인사이트, 의사결정을 위한 데이터 수집, 저장, 분석하려는 데이터 소비자 공동체 빠르게 성장
- 새로운 데이터 집합 수집에도 관여
- 데이터 수집과 빅데이터 분석이 가진 가능성만 보고 싶겠지만,
	- 데이터 거버넌스는 무시해서는 안되는 중요한 고려사항
- 잘 주관된(well-governed)는 데이터 조직에 이득을 가지고 올 수 있음

### 스포티파이의 새 위클리 추천곡 서비스
- data governance에 따라 성장하게 된 시장
- 당시 음악 불법 복제가 음악 산업을 훼손하던 상황에서
	- 음반사들이 작업의 대가를 받는 한 방법으로 출발함
- 사용자에게 노래를 추천할 때, 한곡씩이 아닌
	- 추천곡의 재생목록을 spotify에서 만들어서 제안
	- 매주 월요일에 제시

## 데이터 거버넌스에 대한 [[전일적 접근방식]]
- 개인 식별 정보(Personally Identifying Information, PII)를 포기한 데이터 수집 방식
	- 익명 정보를 통해 신뢰성이 다소 낮더라도 활용한 사례
	- 새로운 데이터 집합으로 기존 데이터 집합 보정, 데이터 품질 향상
- 데이터 거버넌스의 렌즈를 통해 연구팀은
	- 어떤 보고서 데이터를 수집할지 신중히 고려
	- 기업 데이터의 품질 개선
	- 미국 국립 기상청 생성 예보 품질 향상
	- 전반적인 브랜드 기여 향상
- 데이터 거버넌스에 대한 [[전일적 접근방식]](holistic approach)의 결과
- 오늘날 데이터의 분류, 검색, 가용성, 접근성, 무결성, 보안을 관리하는데 어떤 best practice를 적용하고 정책을 마련할 것인가
	- 즉, 데이터 거버넌스를 어떻게 수행할 것인가
- 기업이 데이터 거버넌스를 수행하기 위해 사용할 수 있는 도구와 역량은 강력하고 다양함
	- 처음에는 법률, 규제 준수의 관점뿐 아니라
	- 거버넌스 정책 활용시 성장 촉진 및 비용 감소도 가능함

## 데이터의 신뢰 향상
- 데이터 거버넌스의 궁극적인 목적: 데이터에 대한 신뢰 구축
- 데이터에 대한 이해관계자의 신뢰를 높일 수 있을 때,
	- 구체적으로는 데이터의 수집, 분석 발행, 사용을 이해 관계자가 좀 더 신뢰할 수 있을 때 가치가 있음
- **데이터 신뢰를 확보하기 위한 데이터 거버넌스 전략**
	- 발견성(discoverability)
		- 발견성 자체를 위해서는 데이터 거버넌스가
			- 기술 메타데이터, 계보 정보(lineage information), 비즈니스 용어집(business glossary)을
				- 사용자가 언제라도 사용할 수 있도록 준비해야 함
			- [[비즈니스 크리티컬 데이터]]는 완전해야 함
		- 데이터가 세밀하게 분류되었는지 확인하는,
			- 실수 혹은 악의적인 변경 및 외부 유출(exfiltration)로부터 데이터를 적절히 보호하기 위한 기준 정보 관리(master data management)도 필요함
	- 보안(security)
		- 비즈니스 분야와 데이터 집합의 특성에 따라
		- 규제 준수, 민감한 데이터(e.g. 개인 식별 정보) 관리, 데이터 유출 방지 등이 모두 중요한 사안
	- 설명책임성(accountability, 책무성)
		- 데이터 도메인들의 경계선에 관한 소유권, 설명 책임성에 대한 운영 모델 제공 필요
- 발견성 + 보안 보장시 데이터 자체를 하나의 **상품**으로 취급 가능
	- 이 시점에 설명 책임성이 중요해짐

## 분류 및 접근 제어
- 데이터 거버넌스의 목적, 사업상의 이점을 위해 기업 데이터의 신뢰도를 높히는 것이긴 하나
	- 주요 활동은 여전히 classification(분류)와 access cotrol(접근 제어)와 관련이 있음
	- 주요 역할을 이해하기 위해 위 활동들의 설정을 고려하는 것이 도움이 됨
- 예시: HR 정보를 보호하는 문제
	- 인적 자원, 여러가지 데이터 요소 포함
	- 각 직원의 이름, 고용 일자, 임금 지급 내역, 은행 계좌, 현재 임금 등
	- 각 요소들은 **분류 수준**에 따라 다른 방식으로 보호됨
		- public: 기업 무관한 사람도 접근 가능
		- external: 협력사, 하청업체가 기업의 허락하에 내부 시스템 접근
		- internal: 조직의 모든 임직원 접근
		- restricted: 인가된 임직원만 가능
	- 위 제한이 더 동적일 수도 있음
		- 직원의 현재 임금은 관리자만 볼 수 있고
		- 각 관리자는 자신의 보고서에 필요한 임금 정보만 확인 가능
	- 접근 제어 정책은 사용자가 데이터에 액세스할 때 수행할 수 있는 작업을 규정
		- 새 레코드 생성, 기존 레코드를 읽기/갱신/삭제
- 역할별 구분
	- **[[데이터 주관자]]**
	- 정책 자체의 구현은 데이터베이스 시스템이나 응용 프로그램을 운영하는 팀이 맡을 수 있음
	- [[데이터 스튜어드]]
	-  **[[데이터 사용자]]**
	-  **[[지식 노동자]]**
- 어떤 기업은 개방(open)을 기본 정책으로 둠
	- 비즈니스 데이터의 경우 기업의 모든 지식 노동자 = 인가된 사용자 영역에 속함
- 어떤 기업은 폐쇄(closed)를 기본 정책으로 둠
	- 꼭 필요한 사람만 비즈니스 데이터에 접근하도록
- 어떤 정책을 사용할지는 데이터 거버넌스 위원회가 결정할 일
	- 어떤 접근 방식이 최선인지에 대한 보편적 정답은 없음

## 데이터 거버넌스 대 데이터 활성화 및 데이터 보안

### [[데이터 거버넌스]]

### [[데이터 활성화]]

### [[데이터 보안]]

# 데이터 거버넌스가 점점 더 중요해지는 이유
- 데이터 거버넌스는 주관할 데이터가 생겼을 때부터 존재함
- legacy 데이터 처리 시스템에도 데이터 품질을 보장할 뿐만 아니라
	- 데이터에 대한 접근 제어 방법이 필요함
- 전통적으로 데이터 거버넌스는
	- data source의 유형과 연관된 silo 안에서 수행되는 하나의 IT 기능으로 간주됨
- 대부분의 회사는 데이터 거버넌스를 하나의 부서별 관심사(departmental cencern)로 간주
- 근래에 데이터 거버넌스가 중요해진 이유
	- GDPR, CCPA 같은 규제
		- 보건과 금융을 포함한 몇몇 규제 대상 업계가 아닌 모든 산업에 영향
	- 데이터의 비즈니스 가치에 대한 인식이 높아짐
- 시간이 흐르고 데이터 지형이 바뀌면서
	- 데이터 거버넌스에 대한 접근 방식과 수행 방법도 변화함

## 데이터의 크기가 증가하고 있다
- 기술 발달로 데이터 수집량 증가
	- 예측 분석 능력이 향상된 덕분

## 데이터를 다루거나 열람하는 사람의 수가 기하급수적으로 증가
- 데이터 주도적 의사결정에는 많은 인력이 필요
	- 이를 위해 데이터 파이프라인을 설정하는 기술자
	- 대시보드와 보고서를 보는 이해관계자
- 데이터를 다루고 열람하는 사람이 많을 수록 데이터의 오용 가능성이 커짐
	- 데이터의 접근, 사용을 관리하는 복잡한 시스템의 필요성도 커짐

## 데이터 수집 방법의 발전
- batch 처리 뿐 아니라
	- 개인화된 참여를 제공하는데 실시간 스트리밍, 혹은 가까운 스트리밍 서비스 제공
- 스트리밍의 출현으로 분석 속도가 크게 빨라지면서
	- 침입의 잠재적 위험 역시 증가
	- 보호를 위한 복잡한 설정, 모니터링 필요

### 스포츠 분야의 발전된 데이터 수집
- 예전 스포츠 통계: 승리 횟수, 패배 횟수와 같은 단순 통계
- 요즘은 스포츠 수집 데이터 양과 종류가 증가함
- 미국내 NFL(National Football League)에서 2015년부터 리그 전체에 분석을 사용함
- Big Data Bowl이라는 스포츠 분석 경진대회도 개최함

## 수집되는 데이터의 종류가 늘어나고 있다(좀 더 민감한 데이터도 포함해서)
- 2025년, 모든 사람의 하루 디지털 데이터 참여(digital data engagement)가
	- 1인당 4900건 이상이라고 예측
	- 약 18초마다 한 건의 데이터 상호작용을 의미
- 이런 상호작용의 대부분에서 아주 다양한 **민감한 데이터**가 생성되고 수집됨
	- 신용카드번호, 주민등록번호, 이름, 주소, 건강 상태등
- 극도로 민감한 종류의 데이터 수집이 성행하면서
	- 데이터의 사용, 처리, 열람 권한에 대한 고객(규제 당국)의 우려가 커짐

## 데이터의 용례 확장
- [[데이터 주도적 의사결정]]
- 회사들은 내부적으로 일상 업무 수행에 데이터를 활용하는 것에 머무르지 않고,
	- 고객이 더 나은 결정을 내리도록 돕는데에도 데이터 활용
- 대표적인 예시는 아마존
	- 아래 5가지의 항목을 분석해서 맞춤형 메세지, 추천 상품 제시하는데 활용
		- 고객이 구매한 상품, 조회한 상품, 가상 쇼핑카트에 담은 상품
		- 구매 후 평점, 상품평을 쓴 상품
- 아마존의 용례가 비즈니스 면에서는 합리적이나
	- 그런식으로 활용하는데에는 적합하지 않거나 불법인(민감한) 데이터가 존재함을 주의해야 함
- 민감한 종류의 데이터는 데이터의 취급 방식 뿐 아니라 사용 방식도 중요함
	- 직원 데이터를 회사의 인사 부서에서 내부적으로 사용, 열람 하는 것은 괜찮으나
	- 마케팅 부서에서 사용/열람 하는 것은 부적절

### 더 나은 의사결정을 위한 데이터 활용
- 온라인에서 무언가를 검색할 때 추천 항목이 어떻게 표시되는가

#### Safari Books Online
- 현대적인 분석 도구 활용하여 매출 증가
- 방대한 양의 사용 데이터, 사용자 검색 결과, 추세로부터 가치 발굴
- 그 모든 데이터를 연결해서 sales intelligence를 개선, 매출 증대
- 위와 같은 목적을 실시간으로 달성할 수 있는가
	- 실시간 통찰을 위해 CDN(Content delivery Network)에 대응되는 사용량 데이터를 cloud-native DW로 정기적으로 전송
	- 원래의 사일로 외부에서도 정보 활용하도록 구성
	- ![[Drawing 2025-01-30 18.15.34.excalidraw]]
- Safari books online 팀은 데이터를 심층 분석,
	- 다양한 대시보드와 더 나은 사용자 환경 제공
	- ad-hoc query(임시 질의)도 빠르게 처리하고자 함
- 새로운 분석 덕분에 사용자 지원이 빠르고 간편하며, 고객 만족도가 높아짐
	- 팀이 사용자 주요 정보(IP, 검색한 책 제목)을 거의 실시간으로 얻을 수 있기 때문
- Safari books online이 [[데이터 주도적 의사결정]]을 시작할 때 가장 중요하게 생각한 용례
	- 더 나은 영업 정보를 확보하는 것
		- 예전의 웹 로그에 묻혀 있었거나 아예 사용불가한 모든 데이터를 영업을 위한 단서로 전환
	- 잠재적 독자의 관심도에 대한 평가를 CRM에 통합
		- 실행 가능한 정보로 신속하게 전환

#### California Design Den
- 가격 책정 및 재고 관리 데이터로 의사 결정 프로세스 혁신을 이룸
- 스마트 분석 플랫폼을 활용하여 가격 결정, 재고판매가 빨라지고 수익 증대
- 핵심: 의사결정을 위해 다양한 유형의 데이터를 aggregation 하는 능력
	- 보존해야 할 데이터와 제거해야 할 데이터의 균형을 맞추는 능력 포함

#### 데이터 주도적 의사결정의 과정
- **모든 데이터가 더 나은 의사결정에 가치있는 것은 아님**
- [[데이터 주도적 의사결정]] 과정 확립시 편견을 경계하는 것도 중요
- 목표를 잘 정의해야 하며(처음에는 측정하기 쉬운 목표치 몇가지 설정), 답을 얻고자 하는 고가치 질문(high-value question)을 정해야 함
	- 필요하다면 다시 돌아가서 출발점, 목표, 척도를 재고찰해도 좋음
		- 답은 데이터에서 찾아야 하나, 다양한 관점에서 데이터를 바라보면
		- 답에 중요한 데이터가 무엇인지 파악하는데 도움이 됨
- [[데이터 주도적 의사결정]]에서 value chain의 핵심 부분은 고가치 질문들을 던져서 좀 더 심층적인 통찰을 얻는 것
	- 목적이 영업 정보를 활용하여 매출 정보를 늘리거나
	- 고객 지원, 경험을 개선하거나
	- 악의적인 사용을 감지하여 운영 문제를 방지하거나
	- [[데이터 주도적 의사결정]]은 모든 비즈니스와 운영의 핵심
		- 시중에는 귀중한 데이터의 활용을 돕는 다양한 스마트 도구들 존재
## 데이터 취급에 관한 새로운 규제와 법규
- 데이터 양, 가용량의 증가 => 데이터 자체와 데이터의 수집, 접근, 사용을 규제하려는 요구와 필요성
- 오래전에 생긴 규제
	- HIPAA(Health Insurance Portability and Accountability Act): 건강 보험 이식성 및 책임법
		- 개인 의료 데이터의 수집과 사용을 보호하는 법안, 1996년에 제정
		- 위와 같은 규정은 잘 알려져 있으며, 대상 사업체들이 수십년 동안 준수
			- 민감한 데이터 처리하는 과정, 방법은 정교함
	- GDPR(General Data Protection Regulation): 일반 데이터 보호 규정, EU
- CCPA(California Consumer Privacy Act): 캘리포니아 소비자 개인정보보호법, 미국
	- 새로운 규제
- 위와 같은 규제들은 수많은 회사에 적용되는 데이터 사용, 수집 규제의 두 가지 예일뿐임
- 아직 데이터의 거버넌스를 아직 자신의 데이터 아키텍처 전략에 포함하지 않은 회사도 많음
	- 그래서 예전에는 규제 준수를 고려하지 않던 회사들이 규제를 준수하기 위해
	- 자신의 기술, 업무 프로세스를 수정하는데 어려움을 겪음

## 데이터 사용에 관한 윤리적 우려 사항
- 데이터의 용례 자체도 윤리적 사용이라는 범주에 넣을 수 있으나
	- 요즘은 ML, AI의 신기술이 데이터의 윤리적 사용에 관한 새로운 걱정거리 제공
- 자율주행차에 의한 사고, 히츠버그 사례
	- 책임을 누가질 것인가?
- 위와 같은 사고들은 회사의 홍보 부서에는 엄청난 악몽
- EU 규제 당국은 [[AI 시스템을 신뢰할 수 있다고 간주하려면 충족해야 하는 7대 요건]]을 발표함
- 더 많은 데이터와 안정적인 분석에 힘입어 [[데이터 주도적 의사결정]]을 하려면
	- 위 규제 요건들이 요구하는 것보다 더 집중해서 고려할 필요가 있음

## 데이터 거버넌스의 실제 적용 사례
- 데이터 거버넌스가
	- 접근성과 보안 관리에 쓰이고 있다는 점
	- 데이터 품질을 정면으로 다루어서 신뢰 문제를 해결
	- 거버넌스 구조가 이러한 노력을 성공으로 이끈다는 점

### 발견성, 보안, 설명책임성의 관리
- Capital One: 2019.07, 세계 최대의 소비자 및 중소기업 신용카드 발생사
- 외부인이 Apache Web server의 설정 오류를 악용하여 웹 앱 방화벽을 침해한 사실 발견
- 공격자는 임시 자격증명 획득
	- Capital One 고객의 개인정보가 담긴 파일에 접근 가능
	- 이런식으로 유출된 정보는 Capital One 신용 카드를 신청한 1억 명 이상의 개인에게 영향을 미침
- 이 유출사고의 피해가 더 커지지 못하게 제한한 요인 두가지
	- 유출된 정보는 캐피털 원으로 전송된 신용카드 신청 데이터
		- 이름, 주민번호, 은행 계좌 번호, 주소는 있었으나
		- 공격자가 돈을 훔치는 데 사용할 로그인 자격증명은 없었음
	- 공격자를 FBI가 신속하게 검거
- 문제의 파일들은 어떤 public cloud 저장소의 bucket에 있었음
	- 그 클라우드 저장소는 버킷에 대한 모든 접근을 로그로 남김
	- 그 덕분에 사건 발생 이후 조사관들이 IP 접근 경로 파악, 발원지를 주택 n채로 좁힘
- IT 시스템의 설정 오류로 보안 취약점이 발생하는 것은 어디서나 일어나나,
	- 일반적으로 on-premise 시스템에서 관리자의 자격증명을 훔친 공격자는
		- 시스템 접근 로그를 수정해서 자신의 흔적을 감춤
	- 반면에 public cloud는 공격자가 공용 클라우드 자체를 뚫지 않는 한 그런 로그 수정 불가
- 이 사건에서 배울 몇 가지
	- 데이터 수집은 그 목적이 명확해야 함
		- 또한 데이터를 가능한 한 좁은 범위의 조각들로 나누어 저장하는 것이 좋음
	- [[data_warehouse|데이터 웨어하우스]]에 조직 수준 감사 로그를 활성화 해야함
		- 공격자의 빠른 검거를 도움
	- 모든 열린 포트에 주기적으로 보안 감사 수행 필요
		- 보안 보호 장치를 우회하려는 시도가 있을 때 경고를 받기 위함
	- 문서 안의 민감한 데이터에 대해 추가적인 보안 계층을 적용해야 함
		- [[SSN]]은 [[PII]] 데이터를 식별하고 삭제할 수 있는 인공지능 서비스 활용 하여 마스킹 하거나 토큰화 필요
- 위 4가지 모범관행은 추가적인 안전장치
	- 꼭 필요한 데이터만 수집하고 저장한다면 마스킹은 필요하지 않을 수 있음
- 데이터를 여러 용도로 활용하는 것을 효과적으로 하려면
	- 데이터 요소의 각 속성에 다중 범주에 기반한 태그 or 레이블을 지정하여 적절한 제어 및 보안 적용 필요
- 이를 위해 사업체 내 여러 조직의 협력이 필요함
- 그리고 이처럼 특정 데이터를 고려 대상에서 제거하는 시스템에는
	- 그 나름의 난제와 위험이 따른다는 점도 염두해야 함
- 기업이 수집하고 보유하는 데이터가 증가함에 따라
	- 위와 같은 모범 관행을 잘 이해하고 올바르게 구현 필요
	- 모범관행과 정책, 그리고 그를 구현하는 도구는 데이터 거버넌스의 필수 요소
### 데이터 품질 개선
- 데이터가 조직에 유용하려면 데이터를 신뢰할 수 있어야 함
	- 데이터 품질이 중요
- 데이터 거버넌스에서 대부분의 초점
	- downstream 프로그램이 데이터의 무결성을 신뢰할 수 있게 만드는 것
- 데이터를 조직이 소유하지 않을 때,
	- 데이터가 한 곳에서 다른 곳으로 이동할 때 이를 보장하기가 어려움
- 데이터 거버넌스의 활동으로 데이터 품질을 개선한 좋은 예시
	- USCG(US Coast Guard, 미국 해안 경비대)의 사례
- 데이터가 깨끗하면 깨끗할 수록 좀 더 중요한 용량에 사용량 가능성이 높아짐

#### USCG, 미국 해안경비대가 데이터 품질을 개선한 방법
- 해상 수색 및 구조, 해양 유출 정화, 해양 안전 및 법 집행에 중심을 둔 조직
- AVIS(Authoritative Vessel Identification Service)로 통용되는 시스템에 쓰인 데이터 거버넌스의 개념과 기법
- 데이터의 불일치가 발생함에 따라 USCG는 특정 선박의 위치 및 기타 정보를 현장에서 파악하기 어려웠음
	- 자동화된 도구가 아닌 사람의 개입이 필요한 선박
- 거의 대부분의 경우 문제는 사소한 실수 때문이었지만 바로잡으려면 문제점을 파악하고 관련 해양 공동체에 도움을 청해야 했음
- AVIS가 도움이 된 몇 가지 정성적(qualitative) 사례
	- 어떤 선박이 뭔가를 위반해서 조사를 받거나, 특정 이유로 운항이 금지되었을 경우
	- 그 선박이 동일한 MMSI 번호를 전송하는 여러 선박 중 하나일 경우, 선박의 추적 정보가 중복된 식별 번호로 파악됨
	- USCG의 구조선보다 더 빨리 도움을 줄 수 있는 인근 선박을 찾아야 하는 수색, 구조 상황일 경우
		- 위 결함은 더욱 심각한 문제가 될 수 있음
- 시간이 지남에 따라 파일럿 프로그램에 보고되는 모호한 선박 트랙 수가 급격히 감소
	- 공동체의 노력으로 이루어지는 일
	- 지속적인 유지보수 및 관리


# 데이터 거버넌스의 비즈니스 가치
- 데이터 거버넌스가 전적으로 제어 관행(control practice)인 것은 아님
	- 응집력 있게 구현된 데이터 거버넌스
		- 지식 노동자가 자신의 통찰을 확보하는데 필요한 전략적 요구사항 -> 데이터 쇼핑을 위한 명확한 프로세스 통해 해결
- 조직이 데이터 거버넌스를 하나의 전략적 프로세스(strategic process)로 둔다면
	- [[지식 노동자]]는 업무 수행에 필요한 모든 데이터를 쉽게 찾고 안전하게 접근 권한 신청 가능
		- 그러한 데이터 접근을 위한 명확한 일정, 투명한 승인 절차를 가진 간단한 프로세스를 통해 허락 받음
	- [[데이터 스튜어드|데이터 승인자]]와 [[데이터 주관자]]는 어떤 데이터에 누가 접근할 수 있는지
		- 어떤 데이터가 거버넌스 제어 영역의 '외부'에 있는지(그리고 불일치 사항은 어떻게 처리해야 하는지)
		- 쉽게 파악할 수 있음
	- CIO는 조직내 데이터의 고수준 분석을 검토하여
		- '데이터 총량'이나 '규제 위반 데이터' 같은 정량화 가능한 척도들을 전일적으로 파악할 수 있음
		- 데이터 유출로 인한 조직의 위험 이해 및 관리 가능

## 혁신 촉진
- 좋은 데이터 거버넌스의 전략은 사업체가 데이터에 더 많은 가치를 추출할 수 있게 하는 여러 요소를 결합
	- 목표: 경영 개선, 추가 수익원 발굴, 데이터에서 직접 수익 창출 등 포함
	- 데이터 거버넌스 전략은 기업의 여러 value driver(가치 동인)을 가능하게 만드는 요인
- 잘 작동하는 [[데이터 거버넌스]] 정책은 아래 구성의 조합
	- 프로세스: 적절한 주관하에 데이터를 사용 가능하게 만드는
	- 사람: 정책을 관리하고 조직 전체에서 데이터 접근을 안내하며 필요하다면 사일로를 허무는
	- 도구: 위 노력들을 지원하는
		- 그런 도구들은 머신러닝 기술을 이용해서 데이터를 분류, 발견을 위해 데이터 색인화
- 이상적으로 데이터 거버넌스는
	- 조직의 모든 직원이 거버넌스 규칙하에서
	- 모든 데이터(거버넌스 프로세스에 따라 정의된)에 접근할 수 있게 하며
	- 그러면서도 조직의 위험 대처 태세(risk posture)를 유지
	- 그리고 데이터 거버넌스가 제공하는 추가적인 제거 기능으로
		- 대처 태세가 단지 현재 수준으로 유지되는 것이 아닌, 이전보다 더 개선될 수 있다는 점에서
		- 데이터에 접근할 수 있게 만드는 것은 전적으로 이득
- 모든 [[지식 노동자]]가 잘 통제된 방식으로 데이터에 접근하게 되면
	- 어떠한 질문이 제기되었을 때 개인이 조직 내에 존재하는 데이터를 기반으로
	- 그에 대한 답의 초안을 신속하게 만들어 낼 수 있음 -> 혁신 촉진
- 이는 더 나은 의사결정 및 더 나은 기회 발견 -> 조직의 전체적인 생산성 향상
- 사용 가능한 데이터의 품질, 조직에서 거버넌스가 잘 구현되고 있는지 확인하는 또 다른 척도
- [[데이터 거버넌스]]의 한 부분은 데이터의 quality signal(품질 신호)를 코드화하고 계승하는 방법을 정의하여
	- 데이터의 품질 신호를 보고, 데이터가 큐레이션을 거쳤는지, 정규화가 되었거나 누락되었는지, 제거되었는지 파악 가능
	- 데이터 원본이 얼마나 믿을만 한지도 가늠 가능
- 품질 신호는 데이터의 잠재적 용도에 관한 결정을 내릴 때 필수적인 요소
	- 머신러닝을 위한 훈련 데이터 집합(training dataset)이 품질 신호의 좋은 예

## [[데이터 거버넌스]]와 데이터 분석 민주화의 갈등
- 완전한 데이터 민주화(data democratization)은 데이터 거버넌스와 충돌하는 것으로 간주하는 사람들이 많음
	- 그러한 충돌 관계가 자명한 진리는 아님
- 극단적인 해석의 데이터 민주화
	- 모든 분석가와 지식 노동자가
	- 데이터 등급과 관계없이 모든 데이터에 접근하는 것
- 민감한 데이터에 대해서는 특정한 사람들만 접근 가능해야 하며,
	- 그것도 자신의 구체적인 업무 관련 책임 범위 내에서만 접근 가능해야 함
- 위 측면에서, 데이터 거버넌스는 갈등의 원인이 아닌 갈등 해소의 원동력
- 데이터는 두 가지 계층으로 존재함
	- 하나는 데이터 자체
	- 하나는 데이터 메타데이터(데이터에 대한 데이터)

### 데이터 거버넌스로 달성할 수 있는 세 가지
- 메타데이터 카탈로그에 접근
	- 이 카탈로그에는 관리되는 모든 데이터의 색인이 포함됨
		- 어떤 면에서는 완전한 민주화에 해당
	- 사용자는 이 색인을 검색하여 특정 데이터의 존재 여부 파악 가능
	- 좋은 데이터 카탈로그 = 색인의 범위를 제한하는 특정한 접근 제어 규칙 포함
		- 자신의 권한 밖 데이터에 접근 불가
- 데이터 접근 주관
	- 데이터 획득 프로세스, [[최소 접근 원칙]](principle least access)을 준수하는 방법 포함
- 감사용 기록(audit trail) 관리
	- 데이터 접근 요청, 데이터 접근 승인 주기(data access approval cycle), 승인자([[데이터 스튜어드|data steward]])
	- 승인자 이후의 모든 접근 작업에도 사용할 수 있게 만듦
	- 감사용 기록 자체가 데이터 이므로 반드시 데이터 거버넌스 준수 필요
- 어떤 면에서는 데이터 거버넌스는 [[데이터 민주화]]를 가능하게 만드는 요인으로 작용
	- 그럼으로써 더 많은 [[지식 노동자]]가 더 많은 데이터에 접근할 수 있게 만듦
- 결과적으로 데이터 거버넌스는 비즈니스에서 데이터를 더 쉽고 빠르게 사용할 수 있는 촉진제
- 데이터 거버넌스하에서 서로 다른 업무 단위들이 함께 데이터를 뽑고 분석 -> 심층적인 통찰
	- 조직이 국소적 변화와 전세계적 변화에 기민한 대응 가능

## 위험 관리(도난, 오용, 데이터 손상)
- CIO, [[데이터 스튜어드]]가 고민하는 주제
	- 위험 요소가 무엇이고, 완화 계획이 무엇이며, 잠재적 피해는 무엇인가
- CIO들은 위 관심사들을 질문의 답에 기반하여 자원 할당하는데 사용
	- [[데이터 거버넌스]]는 데이터 효율성, 데이터 가치 추출 같은 여러 주제에 존재하는 데이터에 대한 위험을 관리하기 위한
	- 일단의 도구와 프로세스, 직책을 제공함

### 도난
- 데이터 도난(data theft)또는 데이터 절도는
	- 데이터 자체가 제품이거나
	- 데이터가 가치 창출의 핵심 요소인 조직의 걱정거리
- 조직의 민감한 데이터라고 간주하는 정보를 중심으로 [[데이터 거버넌스]]를 설정하면
	- 주변 데이터나 집계치 등을 좀 더 안심하고 공유 가능하며
	- 비즈니스 효율성이 증가하고 데이터 공유 및 재사용에 대한 장벽 제거 가능

### 오용
- 많은 경우 데이터 오용(misuse)는 데이터를 수집한 목적과 다른 방식으로 사용하는 것을 의미
	- 특히, 잘못된 결론을 뒷받침하기 위해 데이터를 잘못 사용하는 경우 존재
	- 데이터의 원본과 품질, 데이터의 의미에 관한 정보가 부족하면 오용은 흔히 발생
	- 고의로 데이터를 오용하기도 함
- 선의의 목적으로 동의를 얻어 수집한 정보를
	- 의도하지 않은, 때로는 악의적인 목적으로 사용하는 경우가 그런 케이스
- [[데이터 거버넌스]]는 데이터 오용을 여러 계층에 걸쳐 방지 가능함
	- 데이터를 공유하기 전에 신뢰를 확립하는 것
	- 선언적인(declarative) 방법
		- 데이터의 원본과 수집 방법, 의도한 용도들을 컨테이너 안에 선언하는 것
	- 데이터에 접근할 수 있는 기간 제한 -> 잠재적인 오용 방지
		- 데이터를 접근 불가하라는 뜻이 아님
		- 데이터가 존재한다는 사실을 데이터의 목적, 설명과 함께 공유해야 함
		- 그래야 [[데이터 민주화]] 실현 가능

### 데이터 손상
- 데이터 손상(data corruption)은 감지 및 방지가 어려움 -> 방심할 수 없는 위험
- 이 위험은 손상된(때로는 부정확한) 데이터에서 업무상의 운영 결론을 도출할 때 실현
- 데이터 손상은 [[데이터 거버넌스]]의 제어 범위를 벗어난 곳에서 발생할 때가 많음
	- 데이터 입수(ingestion) 오류때문에 발생하거나
	- '깨끗한' 데이터가 손상된 데이터와 결합된 탓에 발생하기도 함
		- 일부 기본값을 포함하여 자동 수정된 부분 -> 데이터를 큐레이션된 데이터로 오인하는 경우 등
- [[데이터 거버넌스]]는 데이터 처리 과정과 계보(structured data, column)에서 기록하고,
	- 또 데이터의 최상위 출처(source)에 대한 신뢰도 혹은 품질을 기록함으로서 문제 대응 가능

## 규제 준수
- 기업들은 다수의 규제가 비즈니스에, 특히 비즈니스가 데이터에 적용될 때 [[데이터 거버넌스]]를 적용함
- 규제(regulation)
	- 조직이 자신이 활동하는 비즈니스 환경 안에서 제 역할을 하려면
	- 반드시 지켜야하는 정책 혹은 방침
	- 흔히 다음 사항 중 하나 이상에 관한 것
		- 세밀한 접근 제어
		- 데이터의 보존과 삭제
		- 감사 기록
		- 민감한 데이터 부류
- [[GDPR]]

### 세밀한 접근 제어에 대한 규제
- 세밀한(fine-grained) 접근제어: 접근제어 + @

#### 접근 제공시 적절한 크기의 컨테이너에 접근하게 하는가
- 이미 요청된 정보가 있는 가장 작은 데이터 컨테이너(테이블, 데이터 집합 등)에만 사용자가 접근할 수 있게 하는가
- 정형 데이터 저장소, 최소 크기 컨테이너는 보통의 경우 개별 테이블 하나
- 데이터 집합 전체, 프로젝트 전체에 대한 접근 허용은 바람직하지 x

#### 접근 제공시 적절한 수준의 접근 권한을 제공하는가
- 데이터 접근 수준(level of access)는 다양함
- 흔히 쓰이는 접근 패턴: **접근 수준을 read/write로 구분. 그 밖의 수준도 가능**
	- e.g. 기여자는 데이터에 새로운 내용을 추가할 수 있게(대신 기존 데이터 수정 불가) 하되
		- 편집자는 데이터를 수정 및 삭제 가능하게 하도록 함
	- **접근 시 일부 데이터를 변형하여 제공하는 식으로 데이터를 보호할 수 있음**
		- e.g. 미국에서 국적 식별 용도로 쓰이는 SSN을 마지막 네 자리만 남기고 가리기
		- e.g. GPS 좌표를 도시, 국가 정도만 식별하도록 정밀도를 낮추기
	- 너무 많은 정보 노출 x & 데이터 공유하는 방식 -> **대칭형(가역적) 암호화를 이용하여 tokenization**
		- 핵심적인 데이터 값(e.g. 개인 식별자)의 고유성은 유지되나
		- 그 개인 식별자의 구체적인 세부사항은 노출되지 않음
- **위에서 언급한 모든 접근 수준(read/write/delete/update, 편집/가리기/토큰화)는 반드시 고려 필요**

#### 접근 제공시 허락된 접근 권한의 유지 기간을 명시하는가
- 일반적으로 사용자는 어떤 이유가 있어서 접근을 요청함
	- 접근 권한이 정당한 근거 없이 필요 이상으로 오래 유지되서는 x
- 규제 기관은 '누가 무엇에 접근하는가'를 물을 것
- 특정 부류의 데이터에 접근할 수 있는 인원의 수를 제어하는 것이 합당하며
	- 그런 제어는 효율적으로 증명 가능

### 데이터 보존과 데이터 삭제
- 데이터의 보존과 삭제에 관한 규제 사항
- **흔히 데이터를 일정 기간 유지하되, 그 이상은 유지하지 x**
	- 금융 거래 규제들은, 금융 사기 수사관이 역추적할 수 있도록
	- 모든 비즈니스 거래 정보들을 최대 7년 동안 보관해야 함
- 반대로, 책임이 따르는 부담(liability)을 줄이면서 빠르게 결론을 도출하려는 목적으로
	- **데이터 보존기관을 짧게 두는 것**이 바람직할 수 있음
	- e.g. 모든 배송 트럭의 위치 정보의 지속적 갱신은 적시(just-in-time) 결정에 유효하나, 일정 시간 유지할 경우 부담
		- 이론적으로 몇 주간 특정 배송 기사의 이동 경로를 그래프화 가능하기 때문

### 감사 로깅
- 규제 정책을 잘 따르고 있다는 증거로서 규제 당국에 제출할 수 있다는 점에서
	- 감사 로그(audit log)를 기록하고 언제라도 불러오는 능력을 갖추는 것이 바람직
- 삭제된 데이터를 자체 제시는 불가능하더라도,
	- 언제 데이터를 생성, 조작, 누구와 공유, 누가 접근 했는지, 언제 만료, 삭제 되었는지 제시 가능
	- 감사자가 정책이 준수 되고 있는지 확인하는 자료가 됨
- 감사 로그는 유용한 감식(forensic) 수단으로 쓰일 수 있음
- 감사 로그가 [[데이터 거버넌스]]에 유용하려면 불변적(immutable) 및 [[write-only]](내부나 외부 단위가 수정 불가)이어야 하며, 장기간 보존되어야 함
- 보존 기간은 가장 엄격한 데이터 보존 정책이 요구하는 기준을 따라야 함
	- 데이터가 삭제됨을 보여주기 위해서는 그보다 길게 보존할 수 있음
- 감사 로그에는 데이터, 데이터 작업 자체에 대한 정보 및 **데이터 관리 시설 주변에서 발생한 작업에 관한 정보**도 포함되어야 함
	- 정책 변경 사항, 데이터 스키마 변경 사항 기록
	- 접근 권한 관리 및 변경 내역 기록
		- 기록에는 변경의 주체(e.g. 데이터 컨테이너, 권한이 부여된 사람)뿐 아니라
		- 그러한 변경을 유발한 사람(해당 활동을 시작한 관리자나 서비스 프로세스) 포함 필요

### 민감한 데이터 부류
- 특정 부류의 데이터를 다른 데이터와 다르게 취급할 것
- 규제들의 주요 관심사: 보호 대상자 그룹 or 특정 종류의 활동
	- 데이터 부류(data class)는 규제의 핵심에 해당
	- 위와 같은 데이터를 법률 용어(e.g. EU 주민에 관한 개인 식별 가능 데이터, 금융 거래 내역)로 표현하기도 함
- 그런 데이터의 어떤 부분이 실제로 처리되는지,
	- 그리고 이 데이터가 정형, 비정형 저장소에 저장된 데이터가 어떤식으로 비교되는지 정확히 파악하는 것은
	- **규제를 준수하는 조직이 파악해야 함**
- 정형 데이터: 하나의 데이터 부류를 필드들의 집합에 바인딩하고(e.g. PII는 그런 필드에 해당)
	- 각 필드에 태그를 지정해서 접근 및 보존을 위한 특정 정책이
	- 해당 필드들에 구체적으로 적용되었는지를 확인하는 식으로 진행
- 그러면 데이터(not (데이터 저장소 & 데이터 조작하는 개인))에 관한 규제 준수 & 세밀한 접근 제어 정책 원칙도 지킴


## 데이터 거버넌스를 고민하는 조직이 고려할 사항들
- 데이터 거버넌스가 운영되는 환경을 반드시 고려해야 함
	- 조직은 유관한 규제들이 어떤 것이고 얼마나 자주 변경되는지
	- 클라우드 배치(deployment)가 조직에 적합한지
	- IT 및 데이터 분석가/소유자에게 필요한 전문지식은 무엇인지

### 규제 및 준수 요건의 변화
- 지난 몇 년 동안 데이터 거버넌스 규제에 대한 관심이 높아짐
- [[GDPR]], CCPA, HIPAA 만큼이나(& PCI 규제) 엄격한 규제로 자리 잡자 대상 조직들이 대응 방안을 마렿나기 시작
- 규제 환경의 변화 -> [[데이터 거버넌스]]에 관해 조직이 경계를 늦추지 말아야 함
	- 고객 정보를 규정대로 처리하지 않아 소송에 휘말리는 등
	- 고객 정보가 중요할 경우 고객 데이터를 처리하는 방식에 주의를 기울여야 함
- 기존 규제 파악 및 기존 규제 조항들이 어떻게 변하는지
	- 업무 수행 방식에 영향을 미칠 수 있는 새로운 규제가 생기지는 않느지 확인 필요
- 기술의 변화 역시 또 다른 과제를 제공함
- 머신러인, AI 덕분에 조직은 미래의 결과, 확률 예측이 가능함
	- 그런 예측 과정에서 이 기술들은 어멍난 양의 새로운 데이터 집합을 만들어냄
- [[데이터 거버넌스]] 관련하여 새로운 예측값을 다루는 방법?
	- 원래의 데이터 집합에 적용한 데이터 거버넌스 정책들을 새 집합에 적용할 것인가
	- 새로운 정책을 따로 마련할 것인가
	- 누가 이 데이터 접근할 것인가
	- 데이터를 얼마나 오래 보존해야 하는가
- 상기 질문들을 고찰하여 답을 내야 함

### 데이터 축적과 조직의 성장
- 인프라(기반구조) 비용이 급격히 감소하고 조직이 성장하면서 데이터 축적(data accumulation)이라는 주제가
	- 데이터 덩치가 급격히 커지는 상황에 빠르고 적절히 대응하는 것이 중요
- 데이터 축적을 통해 조직은 더 많은 출저에서 더 많은 용도로 더 많은 양의 데이터를 수집
- [[데이터 늪]]과 사일로가 많아짐
- 여러가지 난제로 데이터 거버넌스를 적용하기 까다로울 수 있으나 반드시 해야 함
- data lake를 구축하면 여러 문제를 해결할 수 있으나,
	- 이해하고 주관하기가 불가능한 데이터가 너무 많으면 이 역시 [[데이터 늪]]으로 변질 됨

### 데이터를 클라우드로 이전
- 조직들은 on-premise, cloud-infra 두가지 옵션에 대해 대응 및 투자해야 함
- 여러 대형 기업은 관리 대상 데이터를 클라우드로 이전할 계획이 없다고 함
	- 클라우드 데이터 침해시의 결과가 on-premise보다 심하기 때문
	- 금전적 피해 및 평판
- 클라우드 업체는 더 많은 보호장치를 추가하게 됨
	- 클라우드 업체는 거버넌스가 어떻게 구현되는지 모두 보여주어야 함
	- 또한, 고객과의 신뢰 형성 및 고객의 손에 '권력'을 어느정보 부여하는 제어 수단 제공 필요

### 데이터 인프라 전문성
- 인프라 지형의 엄청난 복잡성
- 하이브리드, 다중 클라우드 세상의 [[데이터 거버넌스]] 형상은?
	- 하이브리드 컴퓨팅: on-premise & cloud
	- 다중 클라우드: 둘 이상의 클라우드 공급업체를 사용함
- 데이터가 on-premise, cloud에 존재한다면, 조직 전반에 걸쳐 [[데이터 거버넌스]]를 어떻게 구축할 것인가
	- 단지 구현에 사용할 도구 개선 정도로 해소되지는 않음
	- 조직이 사람과 프로세스, 도구를 함께 고민하고 이 측면을 포괄하는 framework를 정의하는 것으로 시작한다면
	- 데이터 거버넌스를 정의하기 보다 쉬워짐

# 공용 클라우드에서 데이터 거버넌스가 더 쉬운 이유
- 데이터 거버넌스는 위험 관리를 포함
- 실무자는 하기 두 가치의 균형에 대해 고민
	- 접근을 아예 허용하지 않음으로써 얻는 보안상의 장점
	- 조직 내에서 데이터를 자유롭게 하여 생기는 다양한 종류의 의사 결정 및 서비스를 지원하는 민첩함
- 규제 준수를 위해서는 접근 제어, 계보, 보존 정책에 대한 최소한의 요건을 지켜야 함
- 공용 클라우드는 데이터 거버넌스의 구현, 감시, 갱신에 도움이 되는 여러 기능 제공
	- on-premise 시스템에는 그런 기능이 아예 없거나 비용 문제로 사용하기 어려움

## 위치
- [[data locality]] 또는 데이터 국소성
- 상황은 그리 간단하지 않음
	- 사업상의 이유로 어떤 중앙 위치(잠재 고객들의 위치와 가까움)의 데이터 센터 활용
	- 그러나 본사가 독일 회사라면, 관련 규제 때문에 직원에 관한 데이터는 독일에 보관 필요
	- 데이터 전략이 복잡해짐
- 해당 주권 국가(sovereign) 영토에 데이터를 저장할 것을 요구하는 규제 요건이 흔해지고 있음
	- 2016, EU, GDPR에 데이터 주권 조항 포함 승인
	- EU 시민, 주민에 관한 레코드의 저장과 처리 -> 반드시 EU의 법을 따라야 함
- 특정 부류의 데이터(e.g. 호주의 보건 레코드, 독일의 통신 데이터, ...)도 데이터 지역성 규제에 대상이 될 수 있음
	- 이 경우 모든 데이터 처리 및 저장이 해당 국가의 경계 안에서 이루어져야 한다는 것 이상의 규제 존재
	- **주요 공용 클라우드 제공업체들은 위 규제를 지키면서 데이터를 저장하는 기능 제공**
		- 데이터 집합이 EU multi-region 안에 있다고 간단히 표시하기만 하면 redundancy와 규제 준수 모두 해결
		- on-premise 데이터 센터에서 직접 구현하기는 어려움
			- 비즈니스를 수행할, 지역성 규제가 있든 모든 주권 지역에 데이터 센터를 지어야 함
- 트랜잭션 인식 전역 접근(transaction-aware global access)의 중요성
	- 조직은 고객이 어디에 있든 데이터와 응용 프로그램에 접근 가능해야 함
	- 그러나 응용 프로그램, 데이터를 지역 사일로에 함께 배치하는 것으로 규제 준수를 진행한다면 그런 접근 제공이 어려움
	- 제대로 하려면, 응용 프로그램, 사용자 기반으로도 규제 준수 역할 적용이 필요
	- 독자적인 private fiber에서 돌아가며 end-to-end 물리 네트워크와 전역 시간 동기화를 제공하는 공용 클라우드
		- 응용프로그램 구조가 간단해짐

## 표면적 감소
- 규제가 엄격한 업계에서는 데이터 집합, 특히 감사 능력이 요구되는 데이터에 대해 신뢰할 수 있는
	- 단일 진실 공급원(single source of truth)을 보유하는 것이 큰 이득
- 전사적 데이터 웨어하우스(enterprise data warehouse; EDW)를 공용 클라우드에 두면
	- 서로 다른 용례에 대한 서로 다른 데이터 마트(data mart)를 만들 수 있는 능력이 생김
- 컴퓨팅, 저장 분리 가능, 임시 클러스터에서 데이터 접근 가능 환경에서 더 그럴 수 있음
- 그런 데이터 마트의 데이터는 즉석에서 생성되는 EDW view를 통해 제공
- 복사본 유지가 불필요하며, 데이터 정확성 측면에서 그냥 뷰를 검사하는 것만으로 **감사 능력 보장**이 가능함
- 데이터 마트에는 영구적인 저장소가 없으므로 데이터 거버넌스가 간단해짐
	- 저장소가 없기 때문에 데이터 마트 수준에서 **데이터 삭제**와 관련된 규칙을 쉽게 준수 가능
- 모든 규칙은 EDW에서만 지키면 됨
	- 물론 데이터의 적절한 사용 및 제어에 관한 다른 규칙은 수행해야함
- 거버넌스가 필요 없어진 것이 아닌 surface area가 감소한 것

## 임시 컴퓨팅
- 데이터의 원본 혹은 출처를 단일하게 두면서도
	- 현재와 미래의 전사적 응용 프로그램을 계속해서 지원하려면
	- 데이터가 하나의 컴퓨팅 클러스터에 저장되지 않도록
	- 또는 그의 정비례해서 확장되지 않도록 해야 함
- 조직의 업무량이 짧은 시간 안에 급증한다면,
	- 또는 대화식 워크로드(workload)나 비정기적 워크로드를 지원하는 기능이 필요하다면,
	- 무한히 확장할 수 있고 **손쉽게 성능을 급증할 수 있는 컴퓨팅 능력을 저장 아키텍처와는 별도로 마련할 필요가 있음**
	- 이는 데이터 처리 및 분석 아키텍처가 serverless 방식이거나 컴퓨팅과 저장을 명확히 분리한 경우에만 가능
- 데이터 처리와 분석 둘 다 서버리스여야 하는 이유?
	- 데이터를 실제로 활용하려면 먼저 데이터를 준비하고, 정제하고, intelligence tool을 적용하는 단계가 필요함
	- 서버리스 분석 플랫폼의 이점을 살리려면, 이런 도구들이 모두 컴퓨팅과 저장 공간의 분리 및 autoscaling을 지원해야 함
	- 그냥 [[data_warehouse|데이터 웨어하우스]]를 서버리스로 두거나, app을 서버리스 기능 중심으로 구축하는 것으로는 충분하지 않음
	- tooling framework가 서버리스여야 함 -> 클라우드에서만 가능

## 서버리스와 강력함의 조합
- 데이터가 부족한 게 문제가 아니라 데이터를 대규모로 처리할 수 있는 도구가 없는 것이 문제인 기업이 많음
- 구글은 전 세계의 정보를 조직화 하는 것을 자신의 임무로 둠
- 해당 임무 수행을 위해 구글은 데이터 처리 방법 고안 <- 처리 대상 데이터를 보호하고 관리하는 방법 포함
	- 연구용 도구 중 다수는 구글이 서비스에 실제 사용하면서 더 강화됨
	- 이제는 구글 클라우드의 서버리스 도구로서 일반 고객들에게도 제공
- 타 공용 클라우드 제공 업체도 그와 비슷한 도구들이 존재
	- AWS의 Aurora DB, Azure Cosmos DB
	- AWS S3와 Azure의 Cloud Storage: 구글 클라우드 저장소에 대응
	- [[AWS lambda]], Azure Functions: stateless 서버리스 데이터 처리 수행
	- AWS EMR(Elastic MapReduce)와 HDInsight: 구글 클라우드의 Dataproc
	- 책 작성 당시 stateful 서버리스 처리 도구는 구글 클라우드의 Dataflow만 존재하나, 다른 기업도 추가될 것이라 추정
- 위 기능들을 위해서는 부하와 트래픽 급증을 수천 개의 워크로드로 분산시키면서도
	- 서버리스 도구들이 효율적으로 돌아가도록 만들어야 하는데
	- on-premise에서 이와 같이 구현하려면 비용이 너무 많이 듦 -> 비현실적

## 분류된 자원
- 공용 클라우드 제공 업체들은 다양한 과금 옵션을 지원하기 위해 세밀한 자원 labeling과 tagging을 지원함
	- e.g. 데이터 마트에 데이터를 담아둔 조직, 데이터에 대해 컴퓨팅 수행(=컴퓨팅 비용 지급) 조직이 동일할 수도 아닐수도 있음
	- 위와 같을 경우 labeling 및 tagging이 규제 조항들을 준수하는데 도움이 됨
- 이런 기능들에 항목들을 발견하고, 분류하고, 카탈로그화하는 기능이 포함되기도 함
- 자원을 분류하여 lable을 붙이는 능력
	- 자원 식별 및 접근 관리 면에서 중요할 뿐만 아니라
	- 특정 규제 관할 지역에서 데이터의 특정 필드가 개인 식별 정보에 해당하는지를 파악하는데도 중요
	- 그래야 기업의 모든 곳에서 그런 모든 필드에 일관된 정책 적용 가능

## 하이브리드 시스템과 보안
- 이 모든 일의 관건은 **쉽게 적용할 수 있는 일관된 정책을 마련**하는 것
- 일관성과 단일 보안창(single security pane)은 기업 소프트웨어 인프라를 클라우드에 두어서 생기는 주된 장점
	- 하지만 대부분의 기업에서는 모 or 도 접근 방식이 비현실적
- 기업이 edge(IT 시스템 & 현실의 접점. 실측 데이터가 수집되는 현장)에서 물리적 기기(e.g. 휴대용 기기)를 운영한다면
	- 소프트웨어 인프라의 일부도 그런 엣지에서 실행해야 하는 경우가 많음
- 규제 준수를 위해서는 사용중인 장비를 물리적으로 제어해야하는 경우도 존재
- 구형 시스템으로는 클라우드가 제공하는 컴퓨팅, 저장의 분리가 주는 이점 활용이 불가할 수 있음
	- 이럴 경우 on-premise에서 사업을 계속 운영하는게 좋음
	- [[하이브리드 클라우드 시스템]]
- 온프레미스와 클라우드 인프라에 같은 도구를 사용하여 둘을 함께 제어하는 솔루션
	- 클라우드 보안 태세와 정책의 범위 확장 가능
	- e.g. 동일한 프로그램으로 on-premise의 app, 데이터 감사 이후 클라우드에도 수행이 가능
- 위와 같은 역량을 갖추려면 응용 프로그램을 컨테이너화해야 하는데
	- 데이터 거버넌스가 주는 이점이 그런 컨테이너화에 드는 비용을 넘김

# 요약
- 성공적인 데이터 거버넌스 전략 논의시
	- 데이터 아키텍처/파이프라인 구조나 '거버넌스' 작업을 수행하는 도구 고려만으로는 부족
- 거버넌스 도구의 배후에 있는 실제 사람과 '사람 프로세스'도 아주 중요하므로 빼먹지 말고 고려해야 함
- 진정으로 성공적인 거버넌스 전략은
	- 관련 도구뿐만 아니라 사람과 프로세스에 관한 관심사도 반드시 해결해야 함
- 2, 3장: 데이터 거버넌스의 구성요소
- 4장: 데이터 거버넌스가 데이터의 전체 수명 주기에서 어떻게 수행되는지 확인
	- 데이터 입수, 준비, 저장, 보고서와 대시보드, 머신러닝 모델 통합, 데이터 갱신, 삭제
	- 전체 과정에서의 핵심 -> 데이터 품질
	- 새로운 데이터 처리방법 개발, 비즈니스 규칙이 바뀜
- 5장: 데이터 품질을 지속적으로 개선하는 문제
- 2025년에는 기업 데이터의 25%이상이 스트리밍 데이터가 될 것
- 6장: 이동중인 데이터 관리에 따르는 어려운 문제
	- 데이터 전송, 주관 작업에는 출발지와 목적지에서의 데이터 관리, 전송 도중 발생하는 모든 집계, 조작에 대한 관리 포함
	- 데이터 거버넌스는 최종적으로만 정확한([[only eventually correct]]) 저장 시스템에서 데이터가 늦게 도착하는 문제
		- 그것이 계산에 미치는 영향에 대한 난제해결 필요
- 7장: 데이터 보호문제
	- 인증, 보안, 백업 등에 사용할 수 있는 해법들
	- 모니터링을 수행하지 않아 유출, 오용, 사고를 조기에 발견하고 대처하지 못하면 거버넌스도 쓸모 없음
- 8장: 모니터링
- 9장: 논의한 주제 종합
	- 바람직한 문화, 사용자와 사업 기회가 모두 존중되는 문화를 구축하기 위한 모범 관행
- 부록 A: 구글을 데이터 거버넌스 체계의 한 예로 설명
	- 구글이 사용하는 접근 방식의 장단점 지적
	- 그 모든 것을 가능하게 만드는 요소 설명
