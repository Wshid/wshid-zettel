---
date: 2024-07-14
datetime: 2024-07-14T12:18:00
book: 실전_LLM
page:
  - "91"
tags: 
references: 
aliases:
---
# 4.1. 들어가는 글
- 지금까지는 오픈소스/클로즈드 소스 모두 기존 LLM을 그대로 사용
- trasnaformer의 attention 매커니즘과 연산 속도 덕분에
	- 복잡한 문제를 비교적 쉽게 해결 가능
	- 하지만 이것만으로 충분하지 않음
- Fine-Tuning
	- LLM의 모든 잠재력을 활용
- 이미 만들어진 모델을 업데이트 하여 더 높은 품질의 결과를 만듦
- 사용하는 토큰 절약, 더 빠른 답변 가능
- GPT와 같은 광범위한 텍스트로 학습한 모델들은 이미 훌륭한 퓨삿학습 능력을 가지고 있지만,
	- 수 많은 예제를 통해 모델을 미세하게 조정하는 **파인 튜닝**
	- 더 다양한 작업에서 뛰어난 성능 발휘 가능
- 파인 튜닝 모델 활용은 **장기적으로 비용을 절약하는데 효과적**
- 이 장의 목표: 파인튜닝의 전반적인 과정
	- 파인튜닝 훈련을 위한 훈련 데이터 준비
	- 새로운 도는 기존 파인튜닝 모델 훈련
	- 파인튜닝된 모델을 실제 어플리케이션의 통합
- [[데이터 레이블링]]과 같은 몇몇 큰 부분은 다른 곳에서 처리한다고 가정
	- 복잡하고 특별한 작업인 경우 막대한 비용이 들 수 있으나,
	- 예시에서는 데이터 레이블에 의존할 수 있다고 가정
- [[Feature Engineering]]
- 파인 튜닝의 애매한 차이를 이해하고 기술을 익히면
	- LLM의 강력한 기능을 활용하고
	- 특정 요구사항에 맞는 맞춤형 솔루션을 만들 수 있음