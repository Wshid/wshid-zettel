---
date: 2024-06-25
datetime: 2024-06-25 21:20:15
book: 데이터_품질의_비밀
page:
  - "202"
tags: 
references: 
aliases:
---
데이터 사고 관리, 근본 원인 분석, 포스트모템, 사고 커뮤니케이션의 모범 사례 구축 등
- 운영 환경에서 데이터 품질 문제에 실제로 대응하고 해결하는데 필요한 단계 모색
데이터 사고의 원인을 파악한 뒤에 무엇을 해야 할까?
- 대부분의 데이터 조직, 파이프라인을 일시 중지하고 당면한 문제의 근본 원인 파악 작업은 중요
- 단, 데이터 신뢰성을 보장하고, 다른 팀과의 신뢰 관계를 복원하는데 있어 이런 작업은 **빙산의 일각**

# 6.1. 소프트웨어 개발 시 품질 문제 조정
- 대규모 사고를 관리하고, 해결하는 방법에 대한 아이디어를 얻기 위해
	- [[dev_ops]]와 [[sre]]를 참고할 수 있음
- [[dev_ops]] 라이프사이클
	- 계획: 개발 조직은 소프트웨어 목표와 [[Data_SLA_SLO_SLI#What is an SLA?|SLA]]를 이해하기 위해 프로덕트 및 비즈니스 조직과 협력
	- 개발: 새로운 소프트웨어 코드 작성
	- 빌드: 소프트웨어를 테스트 환경에서 릴리즈
	- 테스트: 소프트웨어 테스트
	- 릴리스: 운영 환경에 소프트웨어 릴리스
	- 배포: 소프트웨어를 기존 어플리케이션과 통합, 배포
	- 운영: 소프트웨어를 실행하고 필요에 따라 조정
	- 모니터링: 소프트웨어의 문제 모니터링, 문제 발생시 경고
- 위 사이클은 반복됨
- 소프트웨어 엔지니어링에서 데이터 환경에 이르기까지
	- 사고 관리의 우수 사례 활용으로
	- 비즈니스 분석 요구사항을 충족할 수 있는 능동적이고 확장 가능한 접근 방식으로 [[data_quality|데이터 품질]]문제 해결 가능
- 효과적으로 사고 관리를 하려면 (소프트웨어) 사고로 인한 운영 중단이 지속되지 않도록 제한하고
	- 가능한 빨리, 중단된 비즈니스를 복구해야 함
	- 잠재적 사고에 대한 대응 미리 준비 x -> 실제 상황에는 원칙에 따라 사고 관리를 하는 것이 소용 없을 수 있음
- 사고 관리란
	- 일상적인 엔지니어링 워크플로에서 발생하는 문제를 식별하고
	- 근본 원인을 파악하며, 해결하고, 분석 방지는 전체 작업을 일컬음
	- 프로그래밍 방식으로 버그가 있는 소프트웨어 운영 중단 및 기타 성능 문제를 실시간으로 발견하고 해결

# 6.2. 데이터 사고 관리
- 데이터 시스템은 수백만개의 서로 다른 이유로 중단 가능
	- 그 이유, 방법을 완전히 이해하는 만능 접근 방식은 x
- 어플리케이션을 다운되지 않도록 관리하듯,
	- 데이터 시스템을 동일한 수준으로 잘 다룰 수 있어야 함
- [[data_pipeline|데이터 파이프라인]]에 [[데이터 신뢰성 라이프사이클]]을 적용시킴으로써
	- DE는 비즈니스 영향을 미치기 전에 [[data_quality|데이터 품질]]문제를 보다 원활하게 탐지, 해결, 예방 가능
- 파이프라인을 위한 데이터 사고 관리 워크플로 구축시 단계
	- 사고 감지 및 대응, 근본 원인 분석(RCA; root cause analysis), 사고 해결 및 흠 없는 포스트모템(사후 검토)가 포함됨

### [[데이터 신뢰성 라이프사이클]]
## 6.2.1. 사고 감지
- 적합한 툴, 프로세스 사용시
	- 사고 감지를 데이터 엔지니어링 미 분석 워크플로에 통합 가능
	- 데이터 파프라인 전체의 신선도, 볼륨 및 분포 이슈 발생 탐지 가능
- 문제가 발생할 때 모든 데이터 이해관계자와 최종 사용자에게 적절한 **통신 채널**을 통해 경고 가능
- 데이터를 운영 서비스에 입력하기 전 반드시 테스트를 해야함
- 사고 감지
	- 데이터 파이프라인이 손상되거나 대시보드가 고장날 경우 취할 수 있는 첫번째 단계
	- 데이터 모니터링, 알림을 통해 사고 감지 가능
	- 데이터 파이프라인에서 **수동으로 로직을 구현** 하거나 **특정 임곗값을 기준으로 트리거** 가능
- 사고 감지의 요소
	- [[이상 탐지]]
- **데이터 조직이 사고 관리 문제를 '완벽히 해결'하기 위해 [[이상 탐지]]에만 의존하는 경향 존재 -> 이는 문제**
	- 사고 관리는 결코 '완벽히 해결'할 수 있는 대상이 아님
	- 이상 탐지에만 의존하는 것은 장애의 어떤 한 지점만 파악하는 것
- 단, [[이상 탐지]]는 데이터 신뢰성 라이프사이클에서
	- 매우 중요한 부분이며 데이터 사고 관리 프로토콜의 '감지' 단계를 위한 핵심 툴
	- 그러나 테스트, 버전 관리, 옵저버빌리티, 계보, 자동화 친화적인 데이터 조직에서 사용할 수 있는
		- 다른 기술과 프로세스의 추가 지원 없이 **이상 탐지에 의존하는 것은 큰 문제**임
	- 이상 탐지는 도구일 뿐 만병통치약이 아님
- [[이상 탐지]]는 데이터 상태의 핵심 요소(볼륨, 신선도, 스키마, 분포)가
	- 운영 환경에서 기대치를 충족하지 못할 때 이를 파악하고자 하는 조직에 유용함
	- 엔드 투 엔드로 구현하면 비즈니스 측면에서 매우 가치 있음
- 이상 탐지는 필수적인 출발점이나
	- 근본 원인을 파악하고 영향을 평가하기 위해서는 훨씬 더 많은 작업을 해야 함

## 6.2.2. 사고 대응
- 적절한 사고 대응 -> 효과적인 커뮤니케이션으로 시작하고 마무리
- 표준 사고 대응 방법을 설명하는 두가지 방법
	- [[runbook]]
	- [[playbook]]
- 두 방법 모두 파이프라인이 고장 났을 때 팀이 무엇을 해야하는지 도움이 될 만한
	- 코드, 문서, 링크 및 기타 자료를 포함해야 함
- 기존 사이트 신뢰성 엔지니어링 프로그램에는 서비스에 따라 특정 역할을 위임하는 on-call process가 존재
- 사고 대응자 외에 [[사고 책임자]]도 존재
- 메타데이터
	- 비즈니스 관점에서 데이터 다운타임 사고 발생시 영향을 받는 팀 파악시 유용
	- 엔드 투 엔드 계보와 결합하여 영향을 받는 자산 간의
		- Upstream/downstream 관계를 전달하는 것은 쉽고 빠른 프로세스가 될 수 있음
- 일단 [[data_downtime|데이터 다운타임]]이 발생하면
	- 데이터를 사용하는 업스트림, 다운스트림 사용자 모두에게 데이터 다운타임의 영향을 알리는 것이 중요함

## 6.2.3. 근본 원인 분석
- 이론적으로는 쉬우나 현실적으로는 프로세스를 거치기 어려울 수 있음
- 데이터 사고는 전체 파이프라인에 걸쳐 눈에 잘 띄지 않음
	- 여러 테이블에 영향을 미치기도 함
- 대다수의 데이터 문제의 원인은 하나 이상의 이벤트 일때가 많음
	- 시스템에 공급되는 데이터의 예상치 못한 변화
	- 데이터 변환 로직(ETL, SQL, Spark Job)등의 변경
	- 운영 문제: Runtime Error, 사용 권한 문제, 인프라 장애, 예약 변경 등
- [[아마존의 5 Whys 접근법]]
- 시스템은 한 가지 이유로 중단되는 경우는 거의 없음
- 그에 따라 DE들은 프로세스, 테스트, 데이터 신선도 확인 등의 솔루션 마련
	- [[data_observability|Data Observability]] 시스템의 스키마 변경 알림
- 사전에 문제 식별 불가하다면 이 안전장치가 매우 부적절
- [[데이터 파이프라인에서 근본 원인 분석시 데이터 조직이 취해야할 5단계]]
- 근본 원인 분석은 **데이터 품질 문제**를 거의 실시간으로 해결/방지하는데 강력한 도구가 될 수 있으나
	- 파이프라인이 망가졌을 때 특정 문제로 추적할 수 있는 경우는 거의 없음


## 6.2.4. 사고 해결
- 문제가 발생한 것을 확인하고 미칠 영향을 파악했으면
	- (때로는 근본 원인 분석 전에)
	- 그 다음에는 **문제를 해결하고 적절한 이해관계자에게 다음 단계를 전달하는 것**
- 대부분 '초기 해결책'(파이프라인 일시 중지, 서킷 브레이크)가 있음
- '최종 해결책'은 [[data_downtime|데이터 다운타임]] 사고의 근본 원인을 해결하는 것보다 **영구적인 해결책**을 구현하는 것
	- 이때 다양한 이해관계자가 상황을 쉽게 파악할 수 있도록
		- 슬랙 채널, 이메일 타래, 위키 사이트 등 협업 툴을 통해 사건 형태 전달
- 문제가 해결된 후에는
	- 영향을 받는 당사자에게 다음 단계의 일이 무엇인지 전달하고 며칠 내에 포스트모템을 잡아야 함

## 6.2.5. 흠 없는 포스트모템
> 문제는 시스템이지 코드를 작성한 사람이 아니다
> 실수를 허용하지 않는 것이 시스템의 역할이다
- 데이터 파이프라인은 내결함성이 있어야 함
- DE 조직은 문제를 해결하고 근본 원인을 수행한 후에
	- 발생한 사고의 유형이나 **원인에 관계없이 철저하게 교차 가능 [[postmortem]]을 수행**해야 함
- [[postmortem]]을 수행하기 위한 가이드라인

### 모든 것을 학습 경험으로 구성하자
- 건설적인 대화를 위해 서로를 비난해서는 안됨
- 이러한 경험을 **학습 및 개선**이라는 목표를 기준으로 재구성 하기

### 사고 대응 준비 상태를 평가할 수 있는 기회로 활용하기
- [[runbook]]을 업데이트 하고 모니터링, 알림 및 워크플로 관리 툴 조정

### 각각의 [[postmortem|포스트모템]] 결과를 문서화하고 데이터 조직과 공유
- 문서화를 통해 정보 격차 방지
- 지식의 수준 맞추기
- 퇴사자가 발생하더라도 대응 가능

### SLA 다시 확인하기
- [[Data_SLA_SLO_SLI|SLA]]는 많은 기업에서 특정 벤더, 프로덕트, 내부 조직이 제공할 서비스 수준 및
	- 서비스에 실패할 경우 잠재적인 해결책 정의, 측정할 때 사용하는 방법
- 데이터 시스템이 성숙해지거나 변화함에 따라 [[Data_SLA_SLO_SLI]]를 지속적으로 재검토 해야함
	- 6개월 전에 의미 있었던 [[Data_SLA_SLO_SLI|SLA]]가 이제는 더 이상 의미가 없을 수 있음


# 6.3. 사고 대응 및 완화
- 효과적인 사고 관리 워크플로를 이해하는 것
	- 데이터 이상 및 기타 데이터 다운타임 문제 방지하기 위한 첫 단계
	- 데이터 신뢰성을 제공하는데는 필수적이지 x
- 테스트는 **알려진 미지**에 관한 데이터 품질 문제만 다룸
- 일반적으로 테스트에서는
	- 알려지지 않은 미지의 20%만 다룬다고 함
- [[데이터 신뢰성 라이프사이클]]의 **탐지, 해결** 단계 -> 프로세스의 **반응적** 단계
- [[데이터 신뢰성 라이프사이클]]의 **나머지**단계 -> 프로세스의 **예방적** 단계
- 문제에 대한 근본 원인 파악, 해결 및 포스트모템까지 수행 했다면
	- **문제 관리에 예방적인 접근 방식**을 취할 수 있음
	- [[데이터 신뢰성 스택]]을 구현

## 6.3.1. 사고 관리 루틴 설정
- DE는 데이터 문제를 해결뿐만 아니라
	- **해결의 우선순위**를 정의하여 고민하고,
	- 사고 발생에 따른 상태 전달 부담이 있음
- 대부분의 기업에서는 데이터 문제 해결시, 근본적 채임 소재가 모호하기 때문
- **데이터 신뢰성 SLA**는 전체 팀에서 관리해야 하나
	- 데이터가 손상되었을 경우 SLA를 충족할 수 있도록 지원하는 전담 인력이 필요
	- 해당 역할은 종종 [[사고 책임자]]로 정의됨
- 데이터 조직은 특정 데이터셋의 [[사고 책임자]]를 번갈아가면서 지정해야 함
	- 사고 관리의 우수한 예시를 많이 반들고 이를 반복하도록 확립하는 것은 중요
	- **사고 책임자를 명확히 지정하기**
	- 단, 그러려면 조직 문화 변경이 필요하나
		- 일단 **자동화 기술**에 투자하고 데이터 상태를 지속적으로 확인해도 됨
- 사고 관리자가 데이터 문제의 심각성을 분류하고 평가할 때 취해야 하는 4가지 주요 단계

### 1단계: 적절한 팀원에게 알림 전달하기
- 데이터 사고 대응시
	- 데이터 조직의 구조화 방식 -> 사고 관리 워크플로에 영향 -> 결과적으로 사고 책임 프로세스에 영향
- 데이터 조직 구성원들이 각 사업부에 파견시
	- 사고 대응을 위임하기가 더 쉬움
	- e.g. 마케팅 데이터 및 분석 팀이 모든 마케팅 분석 파이프라인 소유
	- 일반적으로 사업부의 책임자, 데이터 책임자 혹은 최고 데이터 책임자(CDO, Chief Data Officier)에게 보고
- 중앙 집중형 데이터 조직인 경우
	- 각 사업부 상황보다 더 많은 통찰력과 계획 필요
	- CDO 또는 데이터 책임자에게 직접 보고
	- 서로 다른 사업부의 데이터 쿼리문, 사고 관리를 동시에 처리
- 위 두가지 방식 어느 쪽이든
	- 특정 구성원이 소유/유지/관리하는 **데이터 파이프라인의 전용 슬랙 채널**을 운영하면 좋음
	- 관계자를 초대하여 중요한 데이터 상태를 공유 받을 수 있도록
	- 페이저 듀티 혹은 Opsgenie 워크플로를 설정해서 모든 파이프라인 관리

### 2단계: 사고의 심각도 평가하기
- 파이프라인 소유자에게 데이터 문제 알람 발생시 **문제의 심각성 파악 필요**
- 문제 해결을 시작한 후에는
	- 문제의 상태 태그를 운영하면 좋음
		- 수정됨, 예상됨, 조사 중, 조치 필요 없음, false positive
	- 사건의 심각도 파악에 좋음
	- 이해관계자는 영향을 받은 데이터와 관련된 채널에 업데이트를 전달 받아 적절한 조치 가능
- [[Phantom data]]는 최고의 데이터 조직도 쩔절매는 문제
	- 사고를 모두 해결한 뒤 알고보니 **비즈니스에 영향이 없었던** 데이터 문제였던 경우가 허다하기 때문
- 조직에게 중요한 데이터, **우선순위**가 있어야 함
- 조직에 중요한 데이터를 판별하는 방법
	- 계보를 시각화
	- 데이터가 비즈니스에서 어떻게 사용되고 있는가
- 운영 분석
	- 데이터 조직이 회사 전체에서 데이터가 어떻게 사용되는지
	- 데이터 다운타임에 더 취약한 데이터 파이프라인이 어떤 것인지
	- 데이터 자산당 클라우드 스토리지 비용이 얼마나 드는지
	- 데이터 상태에 관한 귀중한 인사이트는 무엇인지
- 운영 분석을 통해 **모든 데이터셋이 어떻게 관련되어 있는지** 가시성을 확보하고 **데이터 소유권** 추적이 가능함
- 데이터 옵저버빌리티 솔루션은 풍부한 계보와 데이터 플랫폼의 운영 분석 탐색 방법을 제공함
- 중요한 사고가 영향을 미쳤는지 팀에서 파악 가능한 경우
	- 다운타임의 심각도 더 잘 이해 가능
	- 금전적 인사이트를 직접적으로 제공하는 데이터에 사고가 영향을 미치는 경우, 우선순위가 높을 수 있음

### 3단계: 상태 업데이트에 관해 자주 이야기하기
- 데이터 조직이 주어진 유형의 사고를 처리하는 방법을 단계별로 안내하는 [[runbook|런북]]을 잘 만들어야 함
- [[runbook|런북]]은 책임의 범위를 확정하고 중복 작업을 줄이기 위해 필요
- '누가 무엇을 하는지'를 파악하고 나면
	- 팀에서 이해관계자가 실시간으로 상태 페이지 업데이트 가능
- 사건 대응 위임이 둘 중 하나로 처리됨
- 아래 두가지 방식은 조직 구조, 리소스 및 우선순위에 가장 적합한 것이 무엇인지에 따라 방식 선택

#### 주어진 시간 동안 모든 사고를 처리하기 위해 대기하는 구성원 지정
- 담당자는 모든 유형의 데이터 사고를 처리할 책임을 가짐
- 어떤 조직은 그들이 관리하는 모든 사고를 처리하는 상근 직원이 있음
- 어떤 조직은 매주 구성원이 일정에 따라 번갈아 처리
#### 특정 테이블을 처리할 책임이 있는 구성원 지정
- 흔히 볼 수 있는 구조
- 구성원은 일상적인 활동을 하면서 할당된 테이블이나 리포트에 관련된 모든 사고를 처리


### 4단계: [[Data_SLA_SLO_SLI|Data SLO 및 SLI]]를 정의하고 조정하여 향후 사고 및 [[data_downtime|다운타임]] 발생 방지하기
- 사고 책임자는 SLO를 설정할 책임은 없으나, SLO를 충족해야 할 책임이 있는 경우가 많음
- [[Data_SLA_SLO_SLI#SLO|SLO]]
- SLA를 정량적으로 측정하는 방법인 SLI는 특정 사용 사례에 따라 다름
- 아래 지표들을 추적함으로써
	- 데이터 조직은 TTD/TTR을 줄이고, 보다 안정적인 데이터 시스템 구축 가능

#### 특정 데이터 자산의 데이터 사고 수(N)
- 외부 데이터 소스에 의존할 가능성이 높음
	- 통제 범위를 벗어날 수 있으나
	- 여전히 다운타임의 중요한 원인
- 일반적으로 측정할 가치가 있음

#### [[탐지시간]](TTD)

#### [[해결시간]](TTR)