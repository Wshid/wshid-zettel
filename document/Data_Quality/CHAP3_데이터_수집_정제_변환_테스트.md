---
date: 2024-05-28
datetime: 2024-05-28 22:30:26
book: 데이터_품질의_비밀
page:
  - "72"
tags: 
references: 
aliases:
---
데이터가 파이프라인에 있기 전과
- 파이프라인에 있는 동안 품질을 관리하는 방법
데이터 실시간 처리 시 사용할 수 있는 **데이터 품질 관리 툴**과 **해당 툴의 이점**을 짚음
[[data_pipeline|데이터 파이프라인]]에서 [[data_quality|데이터 품질]]을 철저히 파악하려면
- 조직에서 지속적으로 운영하는 데이터의 라이프라이클을 **end-to-end**로 살펴보아야 함
데이터 수집 및 정제: production pipeline의 첫번째 단계와 연관
데이터 변환 및 테스트: 데이터 분석을 수행할 수 있게끔 만드는 여정과 관련


# 3.1. 데이터 수집
- 가장 업스트림에 있는 **진입점**
	- 외부 세계의 데이터가 파이프라인에 들어오는 초기 접촉 지점
	- 마치 Docker에서의 Entrypoint
- 진입점의 데이터
	- 그것이 모델링 하는 외부 세계의 전형적인 노이즈, 불규칙성을 모두 포함
	- 제일 원시적
	- e.g. 애플리케이션 로그, 서비스 로그, 클릭 스트림 소스, 라이브 센서, ...
- 데이터 소스는 크게 3가지로 분류됨

## 3.1.1. 애플리케이션 로그 데이터
- 애플리케이션 내의 작업으로 생성된 데이터
- 타임스탬프가 표시되는 이벤트 설명, 애플리케잇녀 소프트웨어 생성 오류, 경고 메세지
- 로그에 포함되거나 불포함 되는 내용이 **개발자**에게 달려 있음
	- 시스템 로그와는 다름
	- 로그 자체가 app 사용에 대한 전체 로그가 아닐 수 있음

### 애플리케이션 로그 데이터 처리시 고려할 점


#### 구조
- app 로그는 단순 **직렬화 텍스트**
	- ASCII 또는 이진 형식으로 사용할 가능성이 높음
- 제약이 거의 없음
- 구조가 매우 다양함

#### 타임 스탬프
- app log text는 `\n`로 구분된 설명이 있는 갭려 이벤트
- 타임스탬프는 이벤트 설명과 달리
	- ISO 표준 형식(`yyyy-mm-ddThh:mm:ss[.mmm]`)이나 유사한 형식으로 표준화 필요

#### 로그 레벨
- 대략적으로 이벤트의 로그 유형 체계화
	- INFO: 순수하게 설명적인 로그
	- WARN: 경고지만 실패는 아님
	- ERROR: app의 프로그래밍 오류

#### 목적
- 로그는 아무렇게나 수집되지 않음
	- 로그 수집 자체도 **비용**이 있기 때문
- 로그를 수집하는 이유?
	- 진단
		- 요청이 얼마나 시간 초과하는가
		- 페이지 로드 속도가 느려지는가
		- 더이상 사용되지 않는 라이브러리를 얼마나 사용하는가
		- 질문에 답변하기 위해 로그를 지능적으로 수집 및 구분 분석 하여 도출
		- 진단 목적으로 로그 데이터 수집시
			- 질문에 대한 답변이 `WARN|ERROR` 로그 일 수 있음
		- 컬렉션의 대부분은 현재 제기하는 **특정 의문**과 관련이 없음
	- 감사
		- 누가 그 요청을 했는가?
		- 몇 번 요청했는가?
		- 시스템은 어떻게 응답했는가?
		- 패턴이 다른가
		- app 내의 이벤트 발생을 기록
		- 대부분의 `INFO`로그가 이 작업에 유용
		- 주로 어플리케이션 세션의 대규모 집계에 활용

## 3.1.2. API 응답
- 하나의 app이 모든 것을 할 수 없기 때문에
	- 특정 기능을 app에 맡김
	- API를 활용
- API는
	- 두 프로그램 사이의 매개체
	- 특정 형식의 **요청**과 **응답**이 필요
	- 목적에 맞는 데이터는 [[반구조화된 데이터]]
- app 로그 외에도 **API Endpoint**에서 가져온 데이터 저장 가능
- 단, 다음 사항에 유의 해야함

### API 응답 사용시 유의점
#### 구조
- 로그처럼 직렬화는 가능하나, **구조화**또는 **반구조화 형식**으로 압축이 풀림
- JSON
	- 우연하지만, 중요한 방식으로 구조에 제약을 받음
	- key-value 혹은 값의 목록
	- 텍스트 스트림일 수도 있는 로그 데이터와 다름
- HTTP
	- HTTP/1.1 와 같은 응답 사양은
	- HTTP 요청 또는 본문에도 JSON|XML 포함 가능

#### 응답 코드
- HTTP 상태 코드
- 다른 코드 표준
	- e.g. HTTP 또는 기타 전송 프로토콜을 사용할 수 있는 `SOAP API`
- 각 코드에는 의미가 있음
	- HTTP 500 응답 속도, 서버 중단 여부를 나타내는 지표
- API 응답 데이터를 저장하는 경우, 이에 대한 고려 필요

#### 목적
- 사용 사례는 API 응답 개체의 어떤 정보가 의미 있는지에 영향을 줄 수 있음
- 특정 상황에서 전송된 정보는 유용하지 않을 수 있음

## 3.1.3. 센서 데이터
- 사물 인터넷 장치, 연구 장비와 같은 센서에서 얻음
- app으로 바로 활용하지는 않음
- 추가로 응용하지 않고 단순히 수집용으로 데이터 전송

### 유의점

#### 노이즈
- 믿을 수 없을 정도로 노이즈가 많음
- 수집 단계에서 초점 x
- 단, 처리량의 중요성은 유의해야 함
- downstream 처리에서
	- 많은 오류값 제거, 평활화 및 기타 변환 수행
- 따라서, 항상 안정적이고 일관된 스트림이 필요

#### 고장 모드
- 센서는 app처럼 똑똑하지 않음
- 센서 실패시 알람을 주지 않을 수 있음
- 수신된 데이터의 양이나, 배치 시간의 시간 [[delta]]와 같은 항목을 영리하게 확인해야 함

#### 목적
- 많은 downstream 작업에 사용
- 오늘날, ML 시스템으로 처리
	- 데이터의 양이 중요할 수 있음
	- 처리량이 중요
- 센서 데이터로 추론 기반 작업시 주의 해야함
	- e.g. 문 앞에서 움직임 감지시 알람 -> 작업의 대기시간이 중요

# 3.2. 데이터 정제
- date cleansing
- 사용 가능한 데이터셋에서 부정확하거나, 대표적이지 않은 데이터 제거
- 데이터 처리 및 데이터 프로덕트 개발 상태에 따라 다양한 종류 존재

### 오류값 제거
- 이상값 제거
	- 표준점수(z점수)와 같은 통계적 기술
	- [[Isolation forest]]와 같은 알고리즘 적용
- 데이터 셋이 큰 경우, 문제 해결 절차에 걸리는 **시간 복잡도**에 유의해야 함

### 데이터셋 특징 평가
- 수집한 데이터의 모든 내용이 다운스트림과 연관 없을 수 있음
	- 그런 섹션은 제거할 것
- 불필요한 데이터 필드 수집은
	- 스토리지 비용 발생
	- 히스토리 추적시 어려움
	- 도메인 지식의 불필요한 확장
	- 분석 복잡도 증가
	- 데이터 품질 저하

### 정규화
- 일부 데이터 포인트는 독립적으로 검사 가능
- 특정 데이터 포인트는 다른 데이터와 비교시 의미 있음
	- 이 경우 정제 및 변환 단계에서 데이터를 정규화 하는것이 도움이 됨
- 정규화에서 자주 사용되는 선택지
	- L1(Manhattan) 정규화
	- L2(Unit) 정규화
	- 평균차분(demeaning) 및 단위 분산
- 최상의 선택이 무엇인지는 데이터 사용 사례에 따라 달라짐

### 데이터 재구성
- 수집한 데이터의 특정 필드 누락
	- cause. 오류 발생이 쉬운 API, 오프라인으로 전환되는 센서
- 일반적으로 누락은 이슈가 없으나, 경우에 따라 모든 필드에 값 지정 필요
	- [[Interpolation|종종 보간법]]
	- [[Extrapolation|외삽법]]
	- 서로 비슷한 데이터의 범주화/분류 등의 기법 사용
		- 기본 수준 범주 레이블 지정 및 자동 식별
			- https://digital.lib.washington.edu/researchworks/handle/1773/43082
- 약간의 노이즈가 포함되겠지만, 오류값 대체 가능

### 시간대 변환
- 표준 시간대 변환, 일종의 정규화로 간주 가능
	- 하지만 시간대 변환 자체가 중요하기 때문에 별도 토픽으로 구성
- 대부분 UTC라는 하나의 기준으로 정리
	- UTC는 표준 시간이지, 시간대는 아님
	- 그리니치 표준시(GMT)를 사용하는 국가는 UTC를 인정하나, 사용하지는 않음
- 시간대 변환 없이 시간대 정보만 수집한다면,
	- 2개의 국제적인 사건 발생시 알기 어려움
- 소프트웨어 버그는 시간대 혼동(e.g. Y2K)으로 추적될 수 있으므로
	- 시간대를 주의 깊게 살펴 UTC로 변환/지정되는지 확인 필요

### 유형 변환
- 대부분의 정형화된 데이터는 유형이 지정됨 => 특정 형식을 따라야 함
	- 하지만 컴퓨팅에서 app이 작동하려면, 형식 변화에 **유동성**을 부여하는 경우가 많음
		- e.g. 부동소수점 => 정수, 문자 => 문자열
- downstream에 특정 유형의 데이터 필요한 경우
	- **정제 프로세스**의 일부로, 한 데이터 유형에서, 다른 데이터 유형으로 값을 **자동**또는 **암시적**으로 변환하는 형식 고려 필요
- 유형 변환은 다른 형식의 데이터 결합하는 경우에도 필수적

# 3.3. 배치 처리 vs 실시간 처리
- 배치 처리
	- 일정 기간 동안 데이터 수집
	- 다량의 데이터를 별개의 패킷으로 '배치'
	- Apache Hadoop
		- 분산 저장 및 대용량 데이터 처리
	- Google BigQuery, Snowflake, Microsoft edge, Amazon redshift
- 실시간 처리
	- 프로세스는 길지만 데이터를 즉시 처리
	- **Apache Kafka**, Amazon Kinesis
	- **Spark**, Flink, Storm, Samza, Flume
	- Databricks, Cloudera

# 3.4. 실시간 처리를 위한 데이터 품질
- 배치 처리와 실시간 처리의 주요 차이점
	- 배치당 처리되는 데이터의 양, 처리 속도
- 배치 처리
	- 지연이 발생하더라도 최대한 많은 양의 데이터를 수집
	- 데이터 품질이 상대적으로 높음
		- 파이프라인에서 주어진 단계에서 데이터의 상태
- 실시간 처리
	- 가능한 한 빨리 데이터를 수집
	- 손실 발생 가능
	- 데이터 품질이 실시간으로 스트리밍 시 오류에 따라 상대적으로 중요함
- 실시간 처리에서 사용되는 데이터 품질 문제를 해결하는 방법?
	- 기존에는 **테스트**를 통해 데이터 품질을 관리
	- 데이터에 대한 가정을 기반으로 작성하나, 모든 결과 설명 불가능
- **테스트는 확장하기 어려웠음**
	- 데이터 품질 문제의 약 20%만 해결 가능했었음
	- [[알려진 미지]]가 존재함
- 아파치 키네시스와 카프카를 활용하여 실시간 처리 시스템의 데이터 품질 관리하기

### [[Amazon Kinesis]]

### [[Apache Kafka]]

스트리밍 시스템의 높은 대기 시간을 고려할 때
- 스트리밍 입력을 다운스트림 시스템으로 직접 **스트리밍**가능
- 단, 이 유형의 분석 데이터는 더 많은 오류를 발생 시킴
- 실시간 또는 거의 실시간으로 이를 이해하는 것이 어려움
	- 분석 사용 사례에 종종 **배치 처리**를 선택하는 이유

[[Apache Kafka|아파치 카프카]]나 [[Amazon Kinesis|아마존 키네시스]] 중에 선택하는 이유는, 조직 요구사항에 따라 달라짐
- SaaS 선호시 키네시스
- 대규모, 구체적인 요구사항시 카프카

데이터 품질 관리를 위해 첫번째 단계는 데이터 정규화임


# 3.5. 데이터 정규화
- 첫 운영 데이터 변환 레이어 => 데이터 정규화 단계
	- 조직에 따라 다르게 붙일 수 있음
- 데이터 변환
	- 소스 형식 -> 목적지 형식, 데이터 이동
- 노이즈, 모호성, 이질성이 최대인 **진입점 데이터**에서 정규화 발생
- 특별히 고려해야할 점이 있음

## 3.5.1. 이종 데이터 소스 처리

### 정규화 지점에서의 데이터 설명
#### 대기 시간 최적화
- 생성 즉시 사용할 수 있도록
- 최종 상태와 상관없이 즉시 파이프라인으로 푸시됨
	- 데이터 배치는 불완전할 것으로 예상해야 함

#### 비위계적인 형식
- 효율성과 사용 편의성을 위해
	- 비위계적이고, 수평적인 스토리지 형식으로 저장
- 깨끗한 창고 + 스키마 + 테이블 x
- S3 버킷과 같은 일부 중앙 저장소에 데이터를 dump할 가능성이 높음

#### 원시 파일 형식
- 진입점 데이터 수평적으로 저장될 뿐 아니라, 스트리밍된 위치에서 원래 파일 형식을 반영할 수 있음
- 애플리케이션 로그 데이터, 센서 데이터를 표 형식으로 변환할 필요 x
- 이는 비쌀 뿐만 아니라, 데이터는 이러한 변환이 필요하지 x

#### 선택적 데이터 필드
- [[data_warehouse|데이터 웨어하우스]] 데이터와 달리
	- JSON과 같은 원시 데이터들은 **선택적 필드**를 가질 수 있음
- 해당 필드의 부재가 `NULL`인지 숫자 `0`인지 등을 추론해야 할 수 있음
- 해당 필드에 따라 기본값이 될 수 있으며
	- 해당 필드가 없는 경우 upstream 처리에 문제가 될 수 있음

#### 이질성
- 위의 모든 특징은 특정한 종류의 **이질성**을 가리킴
- 데이터는 다양한 원본 파일 형식, 다양한 소스로부터 제공되며
	- 동일한 형식의 이진 데이터와 비교하여 처리된 양이 다를 수 있음
- 예측 가능한 종류의 이질성에 대비하여 데이터를 이해하는 것이 이 단계의 핵심
	- 일단 데이터가 저장되고 처리되면, 데이터를 쉽게 변환하여 최대한의 영향을 줄 수 있도록 보장해야 함

##### 이질성 측면: [[data_warehouse|데이터 웨어하우스]] vs [[data_lake|데이터 레이크]]
- 위에서 언급된 내용 대부분 데이터 레이크 형식의 데이터를 설명
- [[data_lake|데이터 레이크]]는 수용할 수 있는 데이터 유형에 대한 제약이 적음
	- 진입점 데이터가 선호하는 스토리지 솔루션 일 수 있음
- 따라서 스트리밍 서비스([[Amazon Kinesis|아마존 키네시스]], [[Apache Kafka|아파치 카프카]])가
	- 서로 다른 소스 위치에서 **비구조화** 및 **반구조화** 데이터를 수집하여
	- 데이터 레이크에 저장한 다음
	- 다음 초기 수준의 운영 변환에 의존하여 이 데이터의 일부를 웨어하우스에서 정형화된 형태로 가져옴
- [[Amazon Kinesis|아마존 키네시스]]용 AWS 람다 함수, [[Apache Kafka|카프카]]스트림즈용 아파치 카프카 소비자가
	- 이러한 종류의 정규화를 적용하는 대표적인 방법
- [[AWS 글루]]는 정기적으로 데이터를 [[data_warehouse|데이터 웨어하우스]]로 이동하는 단계에서 유용함

## 3.5.2. 스키마 검사 및 유형 변환
- 스키마 검사, 유형 변환 - 데이터 정규화에 적용하고자 하는 두가지 기술

### 스키마 검사
- 데이터의 구조가 우리가 기대한 그대로인지 검증하는 과정
- 필수 필드가 있으며 요구하는 형식 데이터가 포함되어 있는가?
	- [[형식 강제]]
- 스키마를 확인해야 하는 이유?
	- 데이터는 패키지 형식으로 제공되는 경우가 많음
	- JSON, CSV 등
- 스키마는 처음으로 데이터를 **패키지 해제** 할 때 무엇을 기대해야 하는지 알려줌
- 스키마 변경은 **데이터 손상의 주요 원인**
- 변경에 따라 발생할 수 있는 오류는 사전에 확인해야 함
	- 예상되는 스키마와
	- 여기에 생길 수 있는 가시적인 변화를 계속 기록하는 작업

### 유형 변환
- 데이터 오류를 일으킬 수 잇음
- 일부 app에서는 오류를 발생시키지 않고
	- 형식을 강제로 지정하거나 암시적으로 캐스팅 할 수 있음
- 문자열 `"4"` -> 정수 `4`: 큰 문제는 아님
- 부동 소수점 `4.00` -> 정수 `4` 문제 발생
- 기본적인 듯 하나 악의적인 버그 발생 가능성 존재

## 3.5.3. 데이터의 구문론적 모호성과 의미론적 모호성
- 데이터의 모호함: 특정 방식으로 중요하게 나타남
- [[구문론적 모호성]]
- [[의미론적 모호성]]

## 3.5.4. 아마존 키네시스 및 아파치 카프카 전반에 걸친 운영 데이터 변환 관리
- 운영 데이터는 원시 상태에서 데이터를  처리하지만,
	- 완전히 맹목적으로 처리해야한다는 의미는 아님
- 많은 데이터 스트리밍 처리 app에서는
	- 기본 제공 알림
	- 필요에 따라 더 복잡한 알림을 구성할 수 있음
- 일반적으로 **운영 변환 단계**에서 수행하는 검사는 이 단계에서 **지연 시간 초과 처리량**에 대한 우선순위와 일치함
- 즉, 이 단계에서는 **데이터 드리프트**와 같은 처리량 집약적인 집계 검사를 피할 수 잇음
	- 대신, 수신 스키마와 과거 스키마를 비교하거나
	- 시간에 따라 검색되는 바이트 볼륨을 추적하는 등
	- 지연 시간이 짧은 검증을 모니터링 목표로 설정해야 함
- 여기서 수행되는 많은 운영 '모니터링'은
	- 수신 데이터가 기존 용량, 스토리지 및 메모리 제약을 초과하지 않도록 하는데 초점을 두기 때문에 [[data_quality|데이터 품질]]을 보장하지 않음

#### [[Amazon Kinesis|아마존 키네시스]]
- [[AWS lambda]]기능을 통해 관리
- 다양한 **전처리**작업을 위해 람다 구성 가능
- 그 편재성을 위해 해당 **전처리**에 일부 데이터 품질 보증을 내포할 수 있음
- `.Net, Go, Java, Node.js, python, ruby`로 작성할 수 있음
- aws console에 업로드만 하면 호출됨
- 실행중인 amazon kinesis instance에 lambda를 연결하려면
	- `connect to a source`
	- `Record preprocessing with AWS Lambda`
- app SQL 코드가 실행되거나 
	- 아마존에 들어오는 데이터의 스키마 스냅샷을 만들기 전에 실행되는
	- 새로운 람다 함수를 만들 수 있다

#### [[Apache Kafka|아파치 카프카]]
- kakfa streams 및 다양한 producer, consumer를 위한 세분화된 설정을 제공함ㄴ
- confluent, instcluster 및 aws가 제공하는 강력한 스트리밍 프레임워크 사용
- [[data_downtime|데이터 다운타임]] 방지를 즉시 처리
- [[data_quality|데이터 품질]]을 위한 다양한 구성 가능성을 제공함
	- [[Schema Registry]]
- kafka stream은 JMX를 통해 **스트리밍 메트릭**을 보고함
	- [[Jconsole]] 활용 가능
- kafka streams java class instnace에서 `KafkaStreams#metrics()` 메서드를 사용하여 메트릭 액세스 가능


# 3.6. 분석 데이터 변환 실행
- [[분석 데이터 변환]]
- 분석 데이터는 몇 가지 측면에서 운영 데이터와 상이함

## 3.6.1. ETL 과정의 데이터 품질 보장
- ETL: 분석 데이터 변환과 동의어로 사용
- ETL: '추출-변환-적재'를 의미
	- 복잡한 데이터를 가진 조직에서 점점 더 보편화되고 있는 3단계 프로세스를 설명
- 추출
	- 일부 업스트림 소스에서 원시 데이터를 내보내고 준비 영역으로 이동
	- MySQL, NoSQL, CRM 시스템, 데이터 레이크 원시 파일
- 변환
	- 준비 영역의 데이터가 데이터 엔지니어의 사양에 따라 결합되고 처리
	- 어떤 경우에는 단순 소스 데이터 복사하는 경우도 존재
	- 어떤 경우에는 변환 집중도가 높을 수 있음
- 로드
	- 변환된 데이터를 [[스테이징 영역]]에서 대상(대개 데이터 웨어하우스의 특정 테이블)로 이동

## 3.6.2. 변환 과정의 데이터 품질 보장
- ETL, ELT의 '변환' 단계는 가장 집약적일 수 있음
- [[ETL]]
- [[ELT]]
- [[ETL]]은 DE가 데이터를 **프로덕션**으로 이동하기 전 검증할 수 있는 기회 제공
	- 단, 데이터를 보다 신속하게 처리하고 테스트 및 모니터링을 적절히 수행하지 않을 경우 데이터 품질 하향

### 소스 데이터를 변환하는 이유
- 스키마 요구 사항에 맞게 필드명 변경
- 소스 데이터 필터링, 집계 및 요약, 중복 제거, 정제 및 통합
- [[유형 변환]] 및 단위 변환 수행
	- e.g. 서로 다른 통화 필드를 모두 미국 달러와 부동 소수점 유형으로 표준화
- 중요한 데이터 필드나 업계 또는 정부 규종을 충족하기 위한 암호화
- [[데이터 거버넌스]] 감시, [[data_quality|데이터 품질]]검사 수행

# 3.7. 테스트 및 경고 알람 시스템
- dbt, wherescape 또는 인포메티카와 같은 ETL 시스템도 오류가 발생하기 쉬움
- 대량 생산 환경에서 이러한 app 수행시
	- 강력한 테스트, 알람 시스템 필요
- 데이터 변환 시스템에는 대부분 데이터 품질을 위한 매커니즘이 내장됨
- 

## 테스트와 데이터 테스트

### 테스트
- 단위 테스트
- 파이프라인 상태에 대한 가시성 메트릭
- 경고

### 데이터 테스트
- 프로덕션 데이터 파이프라인에 들어가기 전
- 데이터 품질 문제를 발견하는데 중요한 역할을 함
- 데이터에 대한 조직의 가정을 검증하는 프로세스
	- **생산 전 또는 생산 중**에 수행
	- e.g. 고유성, `NULL`이 아닌 것을 확인하는 기본 테스트 -> 조직이 소스 데이터에 설정한 기본 가정 테스트 가능
- 데이터가 데이터 조직이 작업할 수 있는
	- 올바른 형식인지, 비즈니스 요구 사항을 충족하는지 확인 가능

### 가장 일반적인 데이터 품질 테스트
- NULL 값 여부
- 용량
	- 데이터를 전부 받았는지
	- 너무 많이 받았는지
	- 너무 적게 받았는지
- 분포
	- 데이터가 허용 범위 내의 값인지
	- 값이 지정된 열의 범위 내에 있는지
- 유니크함
	- 중복된 값이 있는지
- 정보 불변속성
	- 두 개체는 근본적으로 다른지
	- e.g. 순수익 = 매출 - 비용 인가

### 데이터를 테스트하는데 적합한 도구?
- dbt test, great expectations
- 이해 관계자에게 전달하기 전, 데이터 품질문제를 발견하는데 사용

### 데이터 품질 테스트를 위한 사전 작업
- 변화된 데이터를 임시 준비 테이블/데이터셋에 로드
- 테스트를 실행하여 **스테이징 테이블**의 데이터가 운영에 필요한 **임계값**내에 있는지 확인

### 데이터 품질 테스트에 실패하면
- 해당 자산을 담당하는 [[data_engineer|데이터 엔지니어]]나 [[data_analyst|데이터 분석가]]에게 경고가 전송되고 파이프라인은 실행되지 않음
- 이를 통해 데이터 엔지니어는
	- 최종 사용자, 시스템에 영향을 미치기 전에
	- 예상하지 못한 문제 파악 가능
- 데이터 테스트는
	- **변환 전**과 **변환 프로세스**의 각 단계 이후에 수행 가능
