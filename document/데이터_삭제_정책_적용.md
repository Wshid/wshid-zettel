---
date: 2024-05-05
datetime: 2024-05-05 13:46:26
book: 
page: 
tags: 
references: 
aliases:
---
### 해결해야 할 문제
- meta.model_info 테이블에 기록한 데이터 보관 정책에 맞게 데이터를 유지 시켜야 함
- pod을 생성을 해야할까?
- 현재 2개의 pod이 동작중

### 현재 처리 시나리오
- 데이터 처리 시나리오
	- should data run
	- dbt run
	- dbt test
	- distcp(복제하기 위한 두개의 클러스터 설정)
	- dbt alter table(hive connector가 연결되는 설정)

### 처리 시나리오 개선
- 데이터 생성부, 데이터 처리부로 나눈다
- 데이터 생성부
	- dbt(dbt run, dbt test) pod
- 데이터 후속 처리부
	- distcp(distcp)
	- dbt silver meta(Silver Pod)
	- gold remove partitions(Gold Pod)
		- drop partitions
		- delete hdfs
	- silver remove partitions(Silver Pod)
		- drop partitions
		- delete hdfs

### Layered 처리
- 세가지 레벨로 나눔
	- gold
	- warm
	- cold: 데이터 삭제 전 데이터
- cold 데이터 영역: 임시 데이터


### remove partitions의 구현
- var_dt를 받는다(start_dt=20240505000000)
- 
- get_partitions() -> list
	- get_model_info()
		- flow-er-api를 통해 retention_gold 데이터를 가져온다
	- get_retention_start_day()
		- 유지 필요한 최초 날짜 계산
	- get_partitions()
		- show partitions를 통한 삭제 대상 파티션 목록 추출
- remove_partitions()
	- drop_hive_partitions()
		- hive drop partition 명령 수행
	- delete_hdfs_file(move_cold=False)
		- hdfs file delete 수행(or move)
		- soft delete시 cold data 영역으로 이전

